{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RawCTCNet Benchmark/Eval with trained model: CTCLoss of approx. 1.2 (best: 1.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ptang/Desktop/pytorch_models/wavenet-speech\n",
      "/home/ptang/Desktop/pytorch_models/wavenet-speech\r\n"
     ]
    }
   ],
   "source": [
    "# switch to toplevel dir:\n",
    "%cd ~/Desktop/pytorch_models/wavenet-speech/\n",
    "!pwd\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports:\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from warpctc_pytorch import CTCLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import gaussian model, RawCTCNet, sequential decoder:\n",
    "from utils.gaussian_kmer_model import RawGaussianModelLoader\n",
    "from utils.pore_model import PoreModelLoader\n",
    "from modules.raw_ctcnet import RawCTCNet\n",
    "from modules.sequence_decoders import argmax_decode, labels2strings, BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct data generator from gaussian model using the same parameters as we did during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create artificial data model:\n",
    "max_iterations = 1000000 # 1 million examples\n",
    "num_epochs = 100\n",
    "epoch_size = 10000\n",
    "kmer_model_path = \"utils/r9.4_450bps.5mer.template.npz\"\n",
    "batch_size = 8\n",
    "upsample_rate = 3\n",
    "min_sample_len = 80\n",
    "max_sample_len = 90\n",
    "dataset = RawGaussianModelLoader(max_iterations, num_epochs, epoch_size, kmer_model_path, batch_size=batch_size,\n",
    "                                 upsampling=upsample_rate, lengths=(min_sample_len,max_sample_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "   73.8920   73.8920   73.8920  ...   103.5742  103.5742  103.5742\n",
       "  100.0079  100.0079  100.0079  ...     0.0000    0.0000    0.0000\n",
       "  100.7313  100.7313  100.7313  ...     0.0000    0.0000    0.0000\n",
       "              ...                â‹±                ...             \n",
       "  107.1002  107.1002  107.1002  ...     0.0000    0.0000    0.0000\n",
       "  100.4840  100.4840  100.4840  ...     0.0000    0.0000    0.0000\n",
       "   89.7458   89.7458   89.7458  ...     0.0000    0.0000    0.0000\n",
       " [torch.FloatTensor of size 8x243], Variable containing:\n",
       "  4\n",
       "  3\n",
       "  2\n",
       "  1\n",
       "  3\n",
       "  3\n",
       "  1\n",
       "  2\n",
       "  2\n",
       "  1\n",
       "  1\n",
       "  2\n",
       "  1\n",
       "  2\n",
       "  3\n",
       "  4\n",
       "  2\n",
       "  3\n",
       "  3\n",
       "  4\n",
       "  1\n",
       "  3\n",
       "  3\n",
       "  1\n",
       "  2\n",
       "  3\n",
       "  3\n",
       "  1\n",
       "  1\n",
       "  3\n",
       "  2\n",
       "  4\n",
       "  1\n",
       "  1\n",
       "  3\n",
       "  2\n",
       "  1\n",
       "  2\n",
       "  4\n",
       "  1\n",
       "  4\n",
       "  4\n",
       "  3\n",
       "  1\n",
       "  2\n",
       "  4\n",
       "  3\n",
       "  4\n",
       "  1\n",
       "  2\n",
       "  1\n",
       "  4\n",
       "  4\n",
       "  2\n",
       "  1\n",
       "  3\n",
       "  2\n",
       "  4\n",
       "  3\n",
       "  3\n",
       "  2\n",
       "  2\n",
       "  4\n",
       "  1\n",
       "  4\n",
       "  4\n",
       "  1\n",
       "  4\n",
       "  4\n",
       "  4\n",
       "  2\n",
       "  2\n",
       "  4\n",
       "  2\n",
       "  4\n",
       "  2\n",
       "  3\n",
       "  2\n",
       "  3\n",
       "  3\n",
       "  2\n",
       "  2\n",
       "  4\n",
       "  3\n",
       "  2\n",
       "  2\n",
       "  1\n",
       "  2\n",
       "  2\n",
       "  2\n",
       "  3\n",
       "  4\n",
       "  3\n",
       "  2\n",
       "  4\n",
       "  1\n",
       "  1\n",
       "  4\n",
       "  3\n",
       "  3\n",
       "  1\n",
       "  3\n",
       "  2\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  4\n",
       "  1\n",
       "  4\n",
       "  4\n",
       "  3\n",
       "  1\n",
       "  4\n",
       "  4\n",
       "  2\n",
       "  3\n",
       "  4\n",
       "  3\n",
       "  1\n",
       "  4\n",
       "  1\n",
       "  4\n",
       "  3\n",
       "  1\n",
       "  3\n",
       "  4\n",
       "  3\n",
       "  1\n",
       "  3\n",
       "  3\n",
       "  3\n",
       "  2\n",
       "  4\n",
       "  4\n",
       "  2\n",
       "  3\n",
       "  3\n",
       "  4\n",
       "  2\n",
       "  1\n",
       "  4\n",
       "  3\n",
       "  3\n",
       "  3\n",
       "  1\n",
       "  2\n",
       "  3\n",
       "  4\n",
       "  3\n",
       "  1\n",
       "  1\n",
       "  2\n",
       "  1\n",
       "  2\n",
       "  4\n",
       "  2\n",
       "  4\n",
       "  2\n",
       "  4\n",
       "  4\n",
       "  4\n",
       "  4\n",
       "  4\n",
       "  4\n",
       "  2\n",
       "  3\n",
       "  1\n",
       "  3\n",
       "  1\n",
       "  2\n",
       "  1\n",
       "  2\n",
       "  4\n",
       "  4\n",
       "  1\n",
       "  4\n",
       "  1\n",
       "  3\n",
       "  1\n",
       "  1\n",
       "  4\n",
       "  4\n",
       "  3\n",
       "  3\n",
       "  2\n",
       "  3\n",
       "  2\n",
       "  1\n",
       "  1\n",
       "  4\n",
       "  1\n",
       "  1\n",
       "  3\n",
       "  2\n",
       "  1\n",
       "  4\n",
       "  3\n",
       "  4\n",
       "  4\n",
       "  3\n",
       "  3\n",
       "  1\n",
       "  3\n",
       "  1\n",
       "  4\n",
       "  2\n",
       "  2\n",
       "  3\n",
       "  1\n",
       "  2\n",
       "  4\n",
       "  1\n",
       "  2\n",
       "  2\n",
       "  3\n",
       "  2\n",
       "  2\n",
       "  3\n",
       "  4\n",
       "  3\n",
       "  3\n",
       "  1\n",
       "  3\n",
       "  3\n",
       "  4\n",
       "  1\n",
       "  1\n",
       "  4\n",
       "  2\n",
       "  3\n",
       "  4\n",
       "  1\n",
       "  2\n",
       "  4\n",
       "  3\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  3\n",
       "  4\n",
       "  1\n",
       "  4\n",
       "  1\n",
       "  4\n",
       "  1\n",
       "  2\n",
       "  4\n",
       "  4\n",
       "  1\n",
       "  4\n",
       "  4\n",
       "  4\n",
       "  1\n",
       "  1\n",
       "  2\n",
       "  2\n",
       "  2\n",
       "  1\n",
       "  1\n",
       "  4\n",
       "  3\n",
       "  1\n",
       "  1\n",
       "  2\n",
       "  4\n",
       "  3\n",
       "  1\n",
       "  1\n",
       "  3\n",
       "  3\n",
       "  3\n",
       "  2\n",
       "  1\n",
       "  3\n",
       "  3\n",
       "  2\n",
       "  3\n",
       "  3\n",
       "  1\n",
       "  2\n",
       "  4\n",
       "  3\n",
       "  3\n",
       "  2\n",
       "  3\n",
       "  4\n",
       "  2\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  4\n",
       "  2\n",
       "  3\n",
       "  4\n",
       "  2\n",
       "  3\n",
       "  2\n",
       "  3\n",
       "  3\n",
       "  1\n",
       "  1\n",
       "  3\n",
       "  2\n",
       "  4\n",
       "  2\n",
       "  1\n",
       "  4\n",
       "  1\n",
       "  4\n",
       "  4\n",
       "  4\n",
       "  2\n",
       "  2\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  2\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  3\n",
       "  3\n",
       "  1\n",
       "  2\n",
       "  3\n",
       "  2\n",
       "  3\n",
       "  4\n",
       "  4\n",
       "  1\n",
       "  3\n",
       "  1\n",
       "  4\n",
       "  1\n",
       "  1\n",
       "  4\n",
       "  2\n",
       "  2\n",
       "  3\n",
       "  4\n",
       "  4\n",
       "  2\n",
       "  4\n",
       "  4\n",
       "  2\n",
       "  1\n",
       "  2\n",
       "  3\n",
       "  1\n",
       "  2\n",
       "  1\n",
       "  1\n",
       "  4\n",
       "  4\n",
       "  4\n",
       "  1\n",
       "  3\n",
       "  4\n",
       "  1\n",
       "  1\n",
       "  3\n",
       "  4\n",
       "  4\n",
       "  2\n",
       "  4\n",
       "  1\n",
       "  3\n",
       "  1\n",
       "  4\n",
       "  2\n",
       "  2\n",
       "  4\n",
       "  4\n",
       "  2\n",
       "  2\n",
       "  4\n",
       "  3\n",
       "  3\n",
       "  2\n",
       "  3\n",
       "  3\n",
       "  1\n",
       "  3\n",
       "  1\n",
       "  4\n",
       "  2\n",
       "  3\n",
       "  1\n",
       "  2\n",
       "  3\n",
       "  3\n",
       "  3\n",
       "  2\n",
       "  2\n",
       "  4\n",
       "  3\n",
       "  4\n",
       "  1\n",
       "  3\n",
       "  2\n",
       "  2\n",
       "  4\n",
       "  1\n",
       "  1\n",
       "  2\n",
       "  4\n",
       "  2\n",
       "  4\n",
       "  4\n",
       "  3\n",
       "  1\n",
       "  2\n",
       "  4\n",
       "  4\n",
       "  3\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  4\n",
       "  4\n",
       "  1\n",
       "  3\n",
       "  1\n",
       "  1\n",
       "  3\n",
       "  4\n",
       "  2\n",
       "  1\n",
       "  3\n",
       "  2\n",
       "  2\n",
       "  2\n",
       "  2\n",
       "  2\n",
       "  4\n",
       "  2\n",
       "  3\n",
       "  1\n",
       "  4\n",
       "  2\n",
       "  1\n",
       "  3\n",
       "  4\n",
       "  1\n",
       "  2\n",
       "  1\n",
       "  1\n",
       "  2\n",
       "  1\n",
       "  2\n",
       "  3\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  4\n",
       "  4\n",
       "  3\n",
       "  2\n",
       "  1\n",
       "  2\n",
       "  3\n",
       "  2\n",
       "  2\n",
       "  2\n",
       "  1\n",
       "  3\n",
       "  1\n",
       "  2\n",
       "  3\n",
       "  4\n",
       "  3\n",
       "  4\n",
       "  3\n",
       "  4\n",
       "  1\n",
       "  1\n",
       "  2\n",
       "  2\n",
       "  4\n",
       "  4\n",
       "  1\n",
       "  4\n",
       "  4\n",
       "  1\n",
       "  4\n",
       "  4\n",
       "  3\n",
       "  2\n",
       "  4\n",
       "  2\n",
       "  3\n",
       "  2\n",
       "  4\n",
       "  3\n",
       "  3\n",
       "  3\n",
       "  4\n",
       "  3\n",
       "  4\n",
       "  2\n",
       "  2\n",
       "  2\n",
       "  2\n",
       "  2\n",
       "  3\n",
       "  1\n",
       "  1\n",
       "  3\n",
       "  1\n",
       "  4\n",
       "  3\n",
       "  2\n",
       "  3\n",
       "  3\n",
       "  2\n",
       "  4\n",
       "  2\n",
       "  2\n",
       "  3\n",
       "  4\n",
       "  2\n",
       "  4\n",
       "  4\n",
       "  2\n",
       "  1\n",
       "  1\n",
       "  3\n",
       "  2\n",
       "  3\n",
       "  2\n",
       "  1\n",
       "  2\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  3\n",
       "  1\n",
       "  3\n",
       "  4\n",
       "  2\n",
       "  2\n",
       "  4\n",
       "  4\n",
       "  3\n",
       "  3\n",
       "  2\n",
       "  2\n",
       "  3\n",
       "  4\n",
       "  3\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  2\n",
       "  4\n",
       "  2\n",
       "  4\n",
       "  1\n",
       "  2\n",
       "  1\n",
       "  3\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  4\n",
       "  1\n",
       "  3\n",
       "  3\n",
       "  1\n",
       "  1\n",
       "  2\n",
       "  3\n",
       "  1\n",
       "  4\n",
       "  2\n",
       "  4\n",
       "  3\n",
       "  3\n",
       "  1\n",
       "  4\n",
       "  1\n",
       "  2\n",
       "  4\n",
       "  3\n",
       "  4\n",
       "  4\n",
       "  3\n",
       "  2\n",
       "  3\n",
       "  3\n",
       "  1\n",
       "  4\n",
       "  3\n",
       "  1\n",
       "  2\n",
       "  3\n",
       "  3\n",
       "  3\n",
       "  3\n",
       "  2\n",
       "  4\n",
       "  2\n",
       "  4\n",
       "  4\n",
       "  3\n",
       "  3\n",
       "  3\n",
       "  3\n",
       "  2\n",
       "  1\n",
       "  4\n",
       "  2\n",
       "  1\n",
       "  3\n",
       "  1\n",
       "  2\n",
       "  2\n",
       "  2\n",
       "  3\n",
       "  3\n",
       "  4\n",
       "  3\n",
       "  3\n",
       "  1\n",
       "  2\n",
       "  1\n",
       "  4\n",
       "  2\n",
       "  1\n",
       "  1\n",
       "  3\n",
       "  3\n",
       "  3\n",
       "  1\n",
       "  2\n",
       "  1\n",
       "  2\n",
       "  2\n",
       "  3\n",
       "  4\n",
       "  2\n",
       "  1\n",
       "  4\n",
       "  3\n",
       "  3\n",
       "  2\n",
       "  2\n",
       "  3\n",
       "  4\n",
       "  3\n",
       "  1\n",
       "  4\n",
       "  2\n",
       "  2\n",
       "  2\n",
       "  4\n",
       "  3\n",
       "  2\n",
       "  4\n",
       "  4\n",
       "  4\n",
       "  3\n",
       "  2\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  2\n",
       "  3\n",
       "  3\n",
       "  4\n",
       "  2\n",
       "  2\n",
       "  3\n",
       "  3\n",
       "  1\n",
       "  3\n",
       "  3\n",
       "  3\n",
       "  2\n",
       "  3\n",
       "  2\n",
       "  3\n",
       "  1\n",
       "  1\n",
       " [torch.IntTensor of size 691], Variable containing:\n",
       "  89\n",
       "  87\n",
       "  86\n",
       "  82\n",
       "  89\n",
       "  88\n",
       "  82\n",
       "  88\n",
       " [torch.IntTensor of size 8])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect dataset:\n",
    "dataset.fetch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct model with same parameters as during training and load saved models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build model:\n",
    "nfeats = 2048\n",
    "nhid = 512\n",
    "feature_kwidth = 3\n",
    "num_labels = 5\n",
    "num_dilation_blocks = 10\n",
    "dilations = [1, 2, 4, 8, 16] * num_dilation_blocks\n",
    "layers = [(nhid, nhid, 2, d) for d in dilations] + [(nhid, nhid, 3, d) for d in dilations]\n",
    "out_dim = 512\n",
    "is_causal = False\n",
    "ctcnet = RawCTCNet(nfeats, feature_kwidth, num_labels, layers, out_dim, input_kernel_size=2, input_dilation=1,\n",
    "                   softmax=False, causal=is_causal)\n",
    "batch_norm = torch.nn.BatchNorm1d(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load saved model parameters:\n",
    "ctcnet_save_path = \"./runs/gaussian-model/raw_ctc_net.model.adam_lr1e_5.pth\"\n",
    "batchnorm_save_path = \"./runs/gaussian-model/raw_ctc_net.batch_norm.adam_lr1e_5.pth\"\n",
    "map_cpu = lambda storage, loc: storage\n",
    "ctcnet.load_state_dict(torch.load(ctcnet_save_path, map_location=map_cpu))\n",
    "batch_norm.load_state_dict(torch.load(batchnorm_save_path, map_location=map_cpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CTCLoss:\n",
    "ctc_loss_fn = CTCLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to fetch & evaluate model on data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_model():\n",
    "    # use volatile variables for better execution speed/memory usage:\n",
    "    signals, sequences, lengths = dataset.fetch()\n",
    "    signals_var = Variable(signals.data, volatile=True)\n",
    "    sequences_var = Variable(sequences.data, volatile=True)\n",
    "    lengths_var = Variable(lengths.data, volatile=True)\n",
    "    # run networks:\n",
    "    probas = ctcnet(batch_norm(signals_var.unsqueeze(1)))\n",
    "    transcriptions = probas.permute(2,0,1) # need seq x batch x dim\n",
    "    transcription_lengths = Variable(torch.IntTensor([transcriptions.size(0)] * batch_size))\n",
    "    ctc_loss = ctc_loss_fn(transcriptions, sequences_var, transcription_lengths, lengths_var)\n",
    "    avg_ctc_loss = (ctc_loss / transcriptions.size(0))\n",
    "    return (transcriptions, ctc_loss, avg_ctc_loss, sequences.data, lengths.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_target_seqs(seqs, lengths):\n",
    "    \"\"\"Split a flattened array of target sequences into their constituents.\"\"\"\n",
    "    split_seqs = []\n",
    "    labels_parsed = 0\n",
    "    for ll in lengths:\n",
    "        split_seqs.append( seqs[labels_parsed:labels_parsed+ll] )\n",
    "        labels_parsed += ll\n",
    "    return split_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate results against true sequences with argmax and beam search (run these commands in sequence a few times):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTC Loss on whole sequence: 279.40179443359375\n",
      "CTC Loss, averaged per-logit: 1.1404154300689697\n"
     ]
    }
   ],
   "source": [
    "logits, loss, avg_loss, true_seqs, true_seq_lengths = eval_model()\n",
    "print(\"CTC Loss on whole sequence: {}\".format(loss.data[0]))\n",
    "print(\"CTC Loss, averaged per-logit: {}\".format(avg_loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize probabilities with a softmax operation:\n",
    "for k in range(len(logits)):\n",
    "    logits[k,:,:] = torch.nn.functional.softmax(logits[k,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTAAAACGAGCAGCCTAGCCTCTACATACGAAATTGAAACAGCGATCCAGCTCACAGCGACCACCGCTAGGACAAGGCAT\n",
      "AGTTCACCCTTCAATGCTTAATGGGATTCCAGTTAGACGGGCTATTACGTCCGCGGATTCTACGTCCATATAACCGATTTCGGATAATT\n",
      "CACTGTATAGCACTGTTGACTGCCACGACCATGGATAGTTGAGAGTAGCCCCCAAGGGCTGGAACAAGGAAGGAGCTTCCGAA\n",
      "AGACGGTATCATGTCATCTGTCAAAGCAAACGACGGTGGTGACCTAAAAGATGACGAGTTAAGCTTGTGGAGCACATACTG\n",
      "TCTTCACGCAAAGAGGCCGATTTGTGCCTATTGTACACGGGCCTATACCTCACACTTTAATGTCTGCCCAGCTCTACTTATTAGACCC\n",
      "ACAATCGCAAACTGGACTAAAGGAATACCTCAAATACCCATAGTGGTAAGTACCGTCCTCGTTGGTGCAGCTGAGATCAATACAGC\n",
      "TATCCCCTCCTGAATGCTAAACAGTAGGATTCCGGTCGGACCTTGACTGCGTTTCTAGCGGTATCCTTTTTGAGCATCCCGGTGTCGTC\n",
      "CTGGAGACGCGTCTACGAGGATTCCGGCACCCTCCCGACGCCCGTTCATAATAGTGATCTACGGATGACTGCTGTCAGAACCAG\n"
     ]
    }
   ],
   "source": [
    "# print true sequences:\n",
    "true_base_sequences = split_target_seqs(true_seqs, true_seq_lengths)\n",
    "for k in range(len(true_base_sequences)):\n",
    "    print(labels2strings(true_base_sequences[k].unsqueeze(0))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CGAGAAACGAGCAGCATTGCCTATCCATACTACATTGAAACCGAGATCCAGCTCACAGCGCCCACCGCTAGGACAAGCTT\n",
      "TGAAAACCTTCCATTGCTTAATTGGGATTCCAGTTAGCCGGGCTATTACGTACGAGGATTCTAGTTCCATATAACCGATTTCGTAGGCG\n",
      "GAGTCGAGCACTTGTTGACTGCCACGCCCATTGGCGAGTTGAGAGTAGCCACCAAGGGCTTGGAAAAAGGAATGAGATTCCCCCT\n",
      "CTGGGTTTATAATTGGCATATGTCAACGCAAACGACGGTGGTGACATAAAAGATTGACGAGTTAAGCTTGTGGAGCACATCTT\n",
      "TTACCCGCAAAGAGGCAGATTTGTGACTATTGTCCACCGGGCATATACCTCCCACTTTAATTTGTATGCCCAGCTATACTTTATTTTGGACG\n",
      "GTTTCGCAAAATTGGCATAAAGGAATACCTCAAATACCCAGGGTGGTAATTACAGTCCTAGTTGGTGCAGATTGAGAGCCATTGTTCT\n",
      "TTACCCCTCATGACTTGCTAAACCGTAGGATTCAGGTGGGCCCTTGACTTGCGGTTCTCGCGGTATCCTTTTTGAGCATACCGGGTGTCTTA\n",
      "GCTGACGCGTCTACGAGGATTAAGGCACCCTCACGCAGCCCGTTCAGACTAGTGATATACGGATGAATTGCTTGTAATACTTTT\n"
     ]
    }
   ],
   "source": [
    "# argmax decoding: expects (batch, seq, dim) and returns (batch, seq)\n",
    "argmax_decoded = argmax_decode(logits.permute(1,0,2).contiguous().data)\n",
    "argmax_basecalls = labels2strings(argmax_decoded)\n",
    "for k in range(len(argmax_decoded)):\n",
    "    print(argmax_basecalls[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# beam search decoded: expects (batch, dim, seq)\n",
    "beam_search_decoder = BeamSearchDecoder(batch_size=batch_size, num_labels=5, beam_width=6)\n",
    "probas, hyp_seqs = beam_search_decoder.decode(logits.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized probabilities:\n",
      "0.2951114888093909\n",
      "0.29567472496811226\n",
      "0.29745726293447067\n",
      "0.29748092962771044\n",
      "0.2956850947165976\n",
      "0.2951805581851881\n",
      "0.29634259282326214\n",
      "0.2959955643634407\n"
     ]
    }
   ],
   "source": [
    "print(\"Normalized probabilities:\")\n",
    "for k in range(len(probas)):\n",
    "    print(probas[k] / logits.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS>CGAGAAACGAGCAGCATTGCCTATCCATACTACATTGAAACCGAGATCCAGCTCACAGCGCCCACCGCTAGGACAAGCTT<EOS>\n",
      "<SOS>TGAAAACCTTCCATTGCTTAATTGGGATTCCAGTTAGCCGGGCTATTACGTACGAGGATTCTAGTTCCATATAACCGATTTCGTAGGCG<EOS>\n",
      "<SOS>GAGTCGAGCACTTGTTGACTGCCACGCCCATTGGCGAGTTGAGAGTAGCCACCAAGGGCTTGGAAAAAGGAATGAGATTCCCCCT<EOS>\n",
      "<SOS>CTGGGTTTATAATTGGCATATGTCAACGCAAACGACGGTGGTGACATAAAAGATTGACGAGTTAAGCTTGTGGAGCACATCTT<EOS>\n",
      "<SOS>TTACCCGCAAAGAGGCAGATTTGTGACTATTGTCCACCGGGCATATACCTCCCACTTTAATTTGTATGCCCAGCTATACTTTATTTTGGACG<EOS>\n",
      "<SOS>GTTTCGCAAAATTGGCATAAAGGAATACCTCAAATACCCAGGGTGGTAATTACAGTCCTAGTTGGTGCAGATTGAGAGCCATTGTTCT<EOS>\n",
      "<SOS>TTACCCCTCATGACTTGCTAAACCGTAGGATTCAGGTGGGCCCTTGACTTGCGGTTCTCGCGGTATCCTTTTTGAGCATACCGGGTGTCTTA<EOS>\n",
      "<SOS>GCTGACGCGTCTACGAGGATTAAGGCACCCTCACGCAGCCCGTTCAGACTAGTGATATACGGATGAATTGCTTGTAATACTTTT<EOS>\n"
     ]
    }
   ],
   "source": [
    "lookup_dict = {0: '', 1: 'A', 2: 'G', 3: 'C', 4: 'T', 5: '<SOS>', 6: '<EOS>'}\n",
    "for ll in range(len(hyp_seqs)):\n",
    "    print(\"\".join([lookup_dict[lbl] for lbl in hyp_seqs[ll]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
