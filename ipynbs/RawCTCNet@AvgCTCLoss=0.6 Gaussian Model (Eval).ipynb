{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RawCTCNet Benchmark/Eval with trained model: CTCLoss of approx. 0.6 (Best: 0.5548)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ptang/Desktop/pytorch_models/wavenet-speech\n",
      "/home/ptang/Desktop/pytorch_models/wavenet-speech\r\n"
     ]
    }
   ],
   "source": [
    "# switch to toplevel dir:\n",
    "%cd ~/Desktop/pytorch_models/wavenet-speech/\n",
    "!pwd\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports:\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from warpctc_pytorch import CTCLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import gaussian model, RawCTCNet, sequential decoder:\n",
    "from utils.gaussian_kmer_model import RawGaussianModelLoader\n",
    "from modules.raw_ctcnet import RawCTCNet\n",
    "from modules.sequence_decoders import argmax_decode, labels2strings, BeamSearchDecoder\n",
    "from ctcdecode import CTCBeamDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct data generator from gaussian model using the same parameters as we did during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create artificial data model:\n",
    "max_iterations = 1000000 # 1 million examples\n",
    "num_epochs = 100\n",
    "epoch_size = 10000\n",
    "kmer_model_path = \"utils/r9.4_450bps.5mer.template.npz\"\n",
    "batch_size = 6\n",
    "upsample_rate = 6\n",
    "min_sample_len = 80\n",
    "max_sample_len = 90\n",
    "dataset = RawGaussianModelLoader(max_iterations, num_epochs, epoch_size, kmer_model_path, batch_size=batch_size,\n",
    "                                 upsampling=upsample_rate, random_upsample=True, lengths=(min_sample_len,max_sample_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 105.5662  113.7298  106.1436  105.4222  104.3051  110.2205  102.7817\n",
       "  77.0084   79.0295   83.7585   80.3274   98.6562  105.6906  100.3318\n",
       "  92.3886   90.3167   87.6046   88.1289   89.5569   88.2718   94.2597\n",
       " 108.2933  106.1615  102.0731  107.0794  110.6007  107.0333  108.7700\n",
       "  88.2340   86.7097   89.8145   86.4188   89.2195   95.8863   99.0132\n",
       " 103.8159  101.0736  100.9319   98.8079  104.0342   84.1145   81.0289\n",
       "[torch.FloatTensor of size 6x7]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect dataset:\n",
    "signals, bases, lengths = dataset.fetch()\n",
    "signals[:,0:7] # ~ (batch x seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct model with same parameters as during training and load saved models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build model:\n",
    "nfeats = 2048\n",
    "nhid = 512\n",
    "feature_kwidth = 3\n",
    "num_labels = 5\n",
    "num_dilation_blocks = 10\n",
    "dilations = [1, 2, 4, 8, 16] * num_dilation_blocks\n",
    "layers = [(nhid, nhid, 2, d) for d in dilations] + [(nhid, nhid, 3, d) for d in dilations]\n",
    "out_dim = 512\n",
    "is_causal = False\n",
    "ctcnet = RawCTCNet(nfeats, feature_kwidth, num_labels, layers, out_dim, input_kernel_size=2, input_dilation=1,\n",
    "                   softmax=False, causal=is_causal)\n",
    "batch_norm = torch.nn.BatchNorm1d(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load saved model parameters:\n",
    "ctcnet_save_path = \"./runs/gaussian-model/raw_ctc_net.model.adamax_lr2e_4.pth\"\n",
    "batchnorm_save_path = \"./runs/gaussian-model/raw_ctc_net.batch_norm.adamax_lr2e_4.pth\"\n",
    "map_cpu = lambda storage, loc: storage # necessary to move weights from CUDA to CPU\n",
    "ctcnet.load_state_dict(torch.load(ctcnet_save_path, map_location=map_cpu))\n",
    "batch_norm.load_state_dict(torch.load(batchnorm_save_path, map_location=map_cpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CTCLoss:\n",
    "ctc_loss_fn = CTCLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to fetch & evaluate model on data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_model():\n",
    "    # use volatile variables for better execution speed/memory usage:\n",
    "    signals, sequences, lengths = dataset.fetch()\n",
    "    signals_var = Variable(signals.data, volatile=True)\n",
    "    sequences_var = Variable(sequences.data, volatile=True)\n",
    "    lengths_var = Variable(lengths.data, volatile=True)\n",
    "    # run networks:\n",
    "    probas = ctcnet(batch_norm(signals_var.unsqueeze(1)))\n",
    "    transcriptions = probas.permute(2,0,1) # need seq x batch x dim\n",
    "    transcription_lengths = Variable(torch.IntTensor([transcriptions.size(0)] * batch_size))\n",
    "    ctc_loss = ctc_loss_fn(transcriptions, sequences_var, transcription_lengths, lengths_var)\n",
    "    avg_ctc_loss = (ctc_loss / transcriptions.size(0))\n",
    "    return (transcriptions, ctc_loss, avg_ctc_loss, sequences.data, lengths.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_target_seqs(seqs, lengths):\n",
    "    \"\"\"Split a flattened array of target sequences into their constituents.\"\"\"\n",
    "    split_seqs = []\n",
    "    labels_parsed = 0\n",
    "    for ll in lengths:\n",
    "        split_seqs.append( seqs[labels_parsed:labels_parsed+ll] )\n",
    "        labels_parsed += ll\n",
    "    return split_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate results against true sequences with argmax and beam search (run these commands in sequence a few times):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTC Loss on whole sequence: 309.6486511230469\n",
      "CTC Loss, averaged per-logit: 0.6942794919013977\n"
     ]
    }
   ],
   "source": [
    "scores, loss, avg_loss, true_seqs, true_seq_lengths = eval_model()\n",
    "print(\"CTC Loss on whole sequence: {}\".format(loss.data[0]))\n",
    "print(\"CTC Loss, averaged per-logit: {}\".format(avg_loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCATAAAAATAATCGACTAGAGTTCGGGTAACACGGGCTAACCAAGGTAAGCGCGGGCGGCGTTCCGCCGGATAGGTTCTCAATAATC\n",
      "TTCTCCGGTAAGTCATTGGCTTGCTGGACACTTCGATTGCACCTGTAGTAGGGAGCGTATTGTGTGGGATGATGTTCGTGTAGTACTGC\n",
      "AGATGCAGAGAAGATTACACTTATAATTATGTTTCTCACGTATCCAGGCGGAGTGACCGAACTCAAGGGTTAGGGTTAATGA\n",
      "TGAGAGTGCACTTTATTCTCGCTAGTTGTGAATACGAAACTTATACGGCTCGTAATTGTCTCCAATGCCTGAAAGAGCGGTTCTCA\n",
      "GAAGTCTAGGCACACTGGACGTTTAGTTAGGTGCGATTGCCCAAGGTTGGGGAGGACCTTGCCGCCACGCTGTTAGGAACGAAAAAT\n",
      "CCCCGAATCTTGGTCCCTAGAGAGCTAACATGCCGTCGCTAACCACCGTTATGGGAATATCACGCTTCCAATAACGCCATCTTA\n"
     ]
    }
   ],
   "source": [
    "# print true sequences:\n",
    "true_base_sequences = split_target_seqs(true_seqs, true_seq_lengths)\n",
    "for k in range(len(true_base_sequences)):\n",
    "    print(labels2strings(true_base_sequences[k].unsqueeze(0))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ptang/venv/conda3/envs/torch/lib/python3.6/site-packages/ipykernel/__main__.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# normalize probabilities with a softmax operation:\n",
    "temperature = 1.0 # should set this between 0->infty\n",
    "logits = scores / temperature\n",
    "for k in range(len(logits)):\n",
    "    logits[k,:,:] = torch.nn.functional.softmax(logits[k,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTTTTAAAAAATAAATATCCTGGGCGTCGTTAACCCGTGCTAACCCAGGTTAAAGGCGGCGGGGCGGGCGGTTCCCGCCCTGGATAGCTGCTCCATGTAT\n",
      "CGCCGGGGTACGGTTTCCTTTTTTGGGCTTTTGGCTGGGGGACAACTTTTGGATTTTGGACCTGTTCGGTTAGGGAAGGCGCTTTGTTGTTGGGGGGGGATTGGCTGTACGTGGTTAGTTCCC\n",
      "CCGGTCCCGGTTAAGATTTACCACTTTATACTTATATTATCCCGTTCTCCAGAGGTTGGGCCCGCCCCTCCAGGGCTCGGATCCC\n",
      "CTGTGGCCCTTTTTCTCTCGGCTCGTTTGTGGAAATACGAAAACTTTATACGGGCTCGTTTAATTTTTAGTCTCCCATGGCATGGACAGCCGGCGCGGCC\n",
      "TATTCTCGGGCAAACTGGGGGCACGCGGGGTTACGGATTTGGCCCCAGGCGTTGGGGGGAGGCCCTTTGGCCCGCCAAGATGTTTCGAAAGGACCC\n",
      "CCGTCCATCTTTGAGCCCTACGGGAGAGGGCTAAAATGGCCCGTTCCGGCTACCCCAAACAGTATGGGGGGAATCTCCCGGCTTTCCCATAACGCCATCC\n"
     ]
    }
   ],
   "source": [
    "# argmax decoding: expects (batch, seq, dim) and returns (batch, seq)\n",
    "argmax_decoded = argmax_decode(logits.permute(1,0,2).contiguous().data)\n",
    "argmax_basecalls = labels2strings(argmax_decoded)\n",
    "for k in range(len(argmax_decoded)):\n",
    "    print(argmax_basecalls[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CTC beam search: expects (batch, seq, dim)\n",
    "alphabet = [' ', 'A', 'G', 'C', 'T'] # the ordering should match the logit labelling\n",
    "beam_search_decoder = CTCBeamDecoder(alphabet, beam_width=7, blank_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_string(tokens, vocab, seq_len):\n",
    "    return ''.join([vocab[x] for x in tokens[0:seq_len]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# beamres ~ ()\n",
    "# beamscores ~ ()\n",
    "# beamtimes ~ ()\n",
    "# beam_seq_len ~ ()\n",
    "beamres, beamscores, beamtimes, beam_seq_len = beam_search_decoder.decode(logits.permute(1,0,2).data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CTGTAAAATAATATCCTGCGTCGGTAACCCGTGCTAACCCAGGTAAAGCGCGGGCGGCGTTCCGCCTGATAGCTGCTCCATGTG'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ground truth: GCATAAAAATAATCGACTAGAGTTCGGGTAACACGGGCTAACCAAGGTAAGCGCGGGCGGCGTTCCGCCGGATAGGTTCTCAATAATC\n",
    "convert_to_string(beamres[0][0], alphabet, beam_seq_len[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#=======================================\n",
    "#\n",
    "# Aligned_sequences: 2\n",
    "# 1: EMBOSS_001\n",
    "# 2: EMBOSS_001\n",
    "# Matrix: EDNAFULL\n",
    "# Gap_penalty: 10.0\n",
    "# Extend_penalty: 0.5\n",
    "#\n",
    "# Length: 89\n",
    "# Identity:      65/89 (73.0%)\n",
    "# Similarity:    65/89 (73.0%)\n",
    "# Gaps:           6/89 ( 6.7%)\n",
    "# Score: 222.5\n",
    "# \n",
    "#\n",
    "#=======================================\n",
    "\n",
    "EMBOSS_001         1 GCATAAAAATAATCGACTAGAGTTCGGGTAACACGGGCTAACCAAGGT-A     49\n",
    "                      |...||||||||...|| |.||  .||||||.||.|||||||.|||| |\n",
    "EMBOSS_001         1 -CTGTAAAATAATATCCT-GCGT--CGGTAACCCGTGCTAACCCAGGTAA     46\n",
    "\n",
    "EMBOSS_001        50 AGCGCGGGCGGCGTTCCGCCGGATAGGTTCTCAATAATC     88\n",
    "                     ||||||||||||||||||||.|||||.|.|||.||... \n",
    "EMBOSS_001        47 AGCGCGGGCGGCGTTCCGCCTGATAGCTGCTCCATGTG-     84\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CTGACGGTACGTCCTTGGCTTGCTGGACAACTTGGATTGAACCTGTCGTAGGAGCGCTTGTGTGGATGCTGTACGTGTAGTCCC'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ground truth: TTCTCCGGTAAGTCATTGGCTTGCTGGACACTTCGATTGCACCTGTAGTAGGGAGCGTATTGTGTGGGATGATGTTCGTGTAGTACTGC\n",
    "convert_to_string(beamres[1][0], alphabet, beam_seq_len[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#=======================================\n",
    "#\n",
    "# Aligned_sequences: 2\n",
    "# 1: EMBOSS_001\n",
    "# 2: EMBOSS_001\n",
    "# Matrix: EDNAFULL\n",
    "# Gap_penalty: 10.0\n",
    "# Extend_penalty: 0.5\n",
    "#\n",
    "# Length: 91\n",
    "# Identity:      71/91 (78.0%)\n",
    "# Similarity:    71/91 (78.0%)\n",
    "# Gaps:           9/91 ( 9.9%)\n",
    "# Score: 261.0\n",
    "# \n",
    "#\n",
    "#=======================================\n",
    "\n",
    "EMBOSS_001         1 TTCT-CCGGTAAGTCATTGGCTTGCTGGAC-ACTTCGATTGCACCTGTAG     48\n",
    "                       || .|||||.|||.|||||||||||||| ||||.|||||.||||||.|\n",
    "EMBOSS_001         1 --CTGACGGTACGTCCTTGGCTTGCTGGACAACTTGGATTGAACCTGTCG     48\n",
    "\n",
    "EMBOSS_001        49 TAGGGAGCGTATTGTGTGGGATGATGTTCGTGTAGTACTGC     89\n",
    "                     || |||||| .|||||| |||||.|||.||||||||.|.  \n",
    "EMBOSS_001        49 TA-GGAGCG-CTTGTGT-GGATGCTGTACGTGTAGTCCC--     84\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TCGTCCGTAAGATTACACTTATACTTATAGTTATCCCGTCTCCAGAGGTGCCCGCCCTCCAGGCTCGGATCCC'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ground truth: AGATGCAGAGAAGATTACACTTATAATTATGTTTCTCACGTATCCAGGCGGAGTGACCGAACTCAAGGGTTAGGGTTAATGA\n",
    "convert_to_string(beamres[2][0], alphabet, beam_seq_len[2][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#=======================================\n",
    "#\n",
    "# Aligned_sequences: 2\n",
    "# 1: EMBOSS_001\n",
    "# 2: EMBOSS_001\n",
    "# Matrix: EDNAFULL\n",
    "# Gap_penalty: 10.0\n",
    "# Extend_penalty: 0.5\n",
    "#\n",
    "# Length: 95\n",
    "# Identity:      47/95 (49.5%)\n",
    "# Similarity:    47/95 (49.5%)\n",
    "# Gaps:          35/95 (36.8%)\n",
    "# Score: 134.5\n",
    "# \n",
    "#\n",
    "#=======================================\n",
    "\n",
    "EMBOSS_001         1 AGATGCAGAG--------AAGATTACACTTATAATTATGTTTCTCACGTA     42\n",
    "                                       |||||||||||||||.||||..||.||.|||.\n",
    "EMBOSS_001         1 ----------TCGTCCGTAAGATTACACTTATACTTATAGTTATCCCGTC     40\n",
    "\n",
    "EMBOSS_001        43 TCCAGGCGGA-GTGACCGAACTCAAGGGTTAGGGTTAATGA----     82\n",
    "                     ||||    || |||.|||..|||.|||.|        ..||    \n",
    "EMBOSS_001        41 TCCA----GAGGTGCCCGCCCTCCAGGCT--------CGGATCCC     73\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CTAGTTGCCCTTTCTCTCGCTCGTGTGAATACGAAACTTATACGGCTCGTAATTAGTCTCCCATGCATGACAGCGCGCGGCC'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ground truth: TGAGAGTGCACTTTATTCTCGCTAGTTGTGAATACGAAACTTATACGGCTCGTAATTGTCTCCAATGCCTGAAAGAGCGGTTCTCA\n",
    "convert_to_string(beamres[3][0], alphabet, beam_seq_len[3][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#=======================================\n",
    "#\n",
    "# Aligned_sequences: 2\n",
    "# 1: EMBOSS_001\n",
    "# 2: EMBOSS_001\n",
    "# Matrix: EDNAFULL\n",
    "# Gap_penalty: 10.0\n",
    "# Extend_penalty: 0.5\n",
    "#\n",
    "# Length: 90\n",
    "# Identity:      69/90 (76.7%)\n",
    "# Similarity:    69/90 (76.7%)\n",
    "# Gaps:          12/90 (13.3%)\n",
    "# Score: 248.0\n",
    "# \n",
    "#\n",
    "#=======================================\n",
    "\n",
    "EMBOSS_001         1 -TGAGAG-TGCACTTTATTCTCGCTAGTTGTGAATACGAAACTTATACGG     48\n",
    "                      |   || |||.|||| .|||||||.| ||||||||||||||||||||||\n",
    "EMBOSS_001         1 CT---AGTTGCCCTTT-CTCTCGCTCG-TGTGAATACGAAACTTATACGG     45\n",
    "\n",
    "EMBOSS_001        49 CTCGTAATT-GTCTCCAATGCCTGAAAGAGCG-GTTCTCA     86\n",
    "                     ||||||||| ||||||.||||.|||.||.||| |..|   \n",
    "EMBOSS_001        46 CTCGTAATTAGTCTCCCATGCATGACAGCGCGCGGCC---     82\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TCTTCTCGGCAAACTGGCACGTCGTGGTACGATTGCCCCAGCGTGGGAGGCCCTTGCCGCCAAGATGTTTCGAAAGGACCC'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ground truth: GAAGTCTAGGCACACTGGACGTTTAGTTAGGTGCGATTGCCCAAGGTTGGGGAGGACCTTGCCGCCACGCTGTTAGGAACGAAAAAT\n",
    "convert_to_string(beamres[4][0], alphabet, beam_seq_len[4][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#=======================================\n",
    "#\n",
    "# Aligned_sequences: 2\n",
    "# 1: EMBOSS_001\n",
    "# 2: EMBOSS_001\n",
    "# Matrix: EDNAFULL\n",
    "# Gap_penalty: 10.0\n",
    "# Extend_penalty: 0.5\n",
    "#\n",
    "# Length: 97\n",
    "# Identity:      61/97 (62.9%)\n",
    "# Similarity:    61/97 (62.9%)\n",
    "# Gaps:          26/97 (26.8%)\n",
    "# Score: 190.5\n",
    "# \n",
    "#\n",
    "#=======================================\n",
    "\n",
    "EMBOSS_001         1 GAAG---TCTAGGCACACTGG-ACGTTTAGTTAGGTGCGATTGCCCAAG-     45\n",
    "                            |||.||||.||||| |||  |.||  |||.|||||||||.|| \n",
    "EMBOSS_001         1 ----TCTTCTCGGCAAACTGGCACG--TCGT--GGTACGATTGCCCCAGC     42\n",
    "\n",
    "EMBOSS_001        46 GTTGGGGAGGACCTTGCCGCCACGCTGTT-----AGGAACGAAAAAT     87\n",
    "                     ||  ||||||.|||||||||||.|.||||     ||||.|.      \n",
    "EMBOSS_001        43 GT--GGGAGGCCCTTGCCGCCAAGATGTTTCGAAAGGACCC------     81\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GCGTCATCTTGAGCCCTACGAGAGCTAAATGCCGTCGCTACCCCAACAGTATGAATCTCCCGCTTCCCATAACGCCATCC'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ground truth: CCCCGAATCTTGGTCCCTAGAGAGCTAACATGCCGTCGCTAACCACCGTTATGGGAATATCACGCTTCCAATAACGCCATCTTA\n",
    "convert_to_string(beamres[5][0], alphabet, beam_seq_len[5][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#=======================================\n",
    "#\n",
    "# Aligned_sequences: 2\n",
    "# 1: EMBOSS_001\n",
    "# 2: EMBOSS_001\n",
    "# Matrix: EDNAFULL\n",
    "# Gap_penalty: 10.0\n",
    "# Extend_penalty: 0.5\n",
    "#\n",
    "# Length: 91\n",
    "# Identity:      65/91 (71.4%)\n",
    "# Similarity:    65/91 (71.4%)\n",
    "# Gaps:          18/91 (19.8%)\n",
    "# Score: 230.0\n",
    "# \n",
    "#\n",
    "#=======================================\n",
    "\n",
    "EMBOSS_001         1 ----CCCCGAATCTTGGTCCCTA-GAGAGCTAACATGCCGTCGCTAACC-     44\n",
    "                         |     ||||||..||||| ||||||||| ||||||||||||.|| \n",
    "EMBOSS_001         1 GCGTC-----ATCTTGAGCCCTACGAGAGCTAA-ATGCCGTCGCTACCCC     44\n",
    "\n",
    "EMBOSS_001        45 -ACCGTTATGGGAATATCACGCTTCCAATAACGCCATCTTA     84\n",
    "                      ||.| |||  ||||.||.|||||||.|||||||||||.  \n",
    "EMBOSS_001        45 AACAG-TAT--GAATCTCCCGCTTCCCATAACGCCATCC--     80\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Examine model performance on noiseless/constant data:\n",
    "The CTC loss is low, but pairwise alignment gives us ~78% identity to the target sequence; let's check to see if randomness might be contributing to low pct. identity (~80%-85% is competitive).\n",
    "\n",
    "First, data without duration noise, i.e. call `dataset.fetch()` with `random_upsample==False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.random_upsample = False # temporarily turn off random lengths\n",
    "scores, loss, avg_loss, true_seqs, true_seq_lengths = eval_model()\n",
    "print(\"CTC Loss on whole sequence: {}\".format(loss.data[0]))\n",
    "print(\"CTC Loss, averaged per-logit: {}\".format(avg_loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print true sequences:\n",
    "true_base_sequences = split_target_seqs(true_seqs, true_seq_lengths)\n",
    "for k in range(len(true_base_sequences)):\n",
    "    print(labels2strings(true_base_sequences[k].unsqueeze(0))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize probabilities with a softmax operation:\n",
    "temperature = 1.0 # should set this between 0->infty\n",
    "logits = scores / temperature\n",
    "for k in range(len(logits)):\n",
    "    logits[k,:,:] = torch.nn.functional.softmax(logits[k,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# argmax decoding: expects (batch, seq, dim) and returns (batch, seq)\n",
    "argmax_decoded = argmax_decode(logits.permute(1,0,2).contiguous().data)\n",
    "argmax_basecalls = labels2strings(argmax_decoded)\n",
    "for k in range(len(argmax_decoded)):\n",
    "    print(argmax_basecalls[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# beam search decoded: expects (batch, dim, seq)\n",
    "beam_search_decoder = BeamSearchDecoder(batch_size=batch_size, num_labels=5, beam_width=7)\n",
    "probas, hyp_seqs = beam_search_decoder.decode(logits.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Normalized probabilities:\")\n",
    "for k in range(len(probas)):\n",
    "    print(probas[k] / logits.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lookup_dict = {0: '', 1: 'A', 2: 'G', 3: 'C', 4: 'T', 5: '<SOS>', 6: '<EOS>'}\n",
    "for ll in range(len(hyp_seqs)):\n",
    "    print(\"\".join([lookup_dict[lbl] for lbl in hyp_seqs[ll]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again notice that the beam-search output sequences are exactly the same as the argmaxed sequences. Outputs of an EMBOSS run on all six (true seq, pred seq) pairs:\n",
    "```\n",
    "TRUE: TATTCAACTAGCCCCAGACGGTACATCCTAGGACCGAAACATTCGTTTTGTAGAACCTCGCAATTAAACCTGTGTTGGGATGATCG\n",
    "PRED: TGAACTCGGCCCCAGCCGGGTTCCATCACTAGGCCCGACACCATTTCGTATTTGGTTGTCACCCTCGGCCCATTTAAACCTGTTGTTTTTTGGGGGATGCC\n",
    "#=======================================\n",
    "#\n",
    "# Aligned_sequences: 2\n",
    "# 1: EMBOSS_001\n",
    "# 2: EMBOSS_001\n",
    "# Matrix: EDNAFULL\n",
    "# Gap_penalty: 10.0\n",
    "# Extend_penalty: 0.5\n",
    "#\n",
    "# Length: 106\n",
    "# Identity:      68/106 (64.2%)\n",
    "# Similarity:    68/106 (64.2%)\n",
    "# Gaps:          25/106 (23.6%)\n",
    "# Score: 192.5\n",
    "# \n",
    "#\n",
    "#=======================================\n",
    "\n",
    "EMBOSS_001         1 TATTCAACT-AGCCCCAGACGG--TACATC-CTAGGACCGAAAC--ATTC     44\n",
    "                        |.|||| .|||||||.|||  |.|||| |||||.||||.||  .|||\n",
    "EMBOSS_001         1 ---TGAACTCGGCCCCAGCCGGGTTCCATCACTAGGCCCGACACCATTTC     47\n",
    "\n",
    "EMBOSS_001        45 G--TTTTGTAG--AACCTCG---CAATTAAACCTG-TGTT------GGGA     80\n",
    "                     |  |||.||.|  |.|||||   ||.||||||||| ||||      ||||\n",
    "EMBOSS_001        48 GTATTTGGTTGTCACCCTCGGCCCATTTAAACCTGTTGTTTTTTGGGGGA     97\n",
    "\n",
    "EMBOSS_001        81 TGATCG     86\n",
    "                     ||..  \n",
    "EMBOSS_001        98 TGCC--    101\n",
    "#---------------------------------------\n",
    "#---------------------------------------\n",
    "TRUE: ACTCAGAGGCAATGACGACAAAACGGGATAGCATTACTGGTGGCGGACTCGTATACCTAGGGAGCATGATGCGCATGTCATAAGAGTGG\n",
    "PRED: ACCAGGCCCATGGAAGCCCAAACGTGGAGTCCATTTCCTGGGTTGGGAGGGCCTGGTTATCCATCGGGAAGCCATGGGCTGCGCCATGGGTCCATAAGCCCAC\n",
    "#=======================================\n",
    "#\n",
    "# Aligned_sequences: 2\n",
    "# 1: EMBOSS_001\n",
    "# 2: EMBOSS_001\n",
    "# Matrix: EDNAFULL\n",
    "# Gap_penalty: 10.0\n",
    "# Extend_penalty: 0.5\n",
    "#\n",
    "# Length: 108\n",
    "# Identity:      65/108 (60.2%)\n",
    "# Similarity:    65/108 (60.2%)\n",
    "# Gaps:          24/108 (22.2%)\n",
    "# Score: 115.0\n",
    "# \n",
    "#\n",
    "#=======================================\n",
    "\n",
    "EMBOSS_001         1 ACTCAGAGGCAAT-GACGACAAAACG-GGATAGCA-TTACTGG--TGG--     43\n",
    "                     || ||| |.|.|| ||.|.|.||||| |||...|| ||.||||  |||  \n",
    "EMBOSS_001         1 AC-CAG-GCCCATGGAAGCCCAAACGTGGAGTCCATTTCCTGGGTTGGGA     48\n",
    "\n",
    "EMBOSS_001        44 CGGACTCG-TATACCTAGGGA--GCAT--GATGCG-CATG---TCATAAG     84\n",
    "                     .||.||.| |||.|.|.||||  .|||  |.|||| ||||   .||||||\n",
    "EMBOSS_001        49 GGGCCTGGTTATCCATCGGGAAGCCATGGGCTGCGCCATGGGTCCATAAG     98\n",
    "\n",
    "EMBOSS_001        85 ---AGTGG     89\n",
    "                        |.   \n",
    "EMBOSS_001        99 CCCAC---    103\n",
    "#---------------------------------------\n",
    "#---------------------------------------\n",
    "TRUE: GCCGGGACGGATGCAACTAGCCCCTATCAGCGTTTGCTTTTACCGCGTGCCAACTTCTGTGCGTCATTGACGATCAGCCCTTGAG\n",
    "PRED: TGGGGCATGATGGCCCACTCGGCCCATCTCCGCGTTCCTTATTTTTCCCGGCGGTGGCCAACTTTCTATTGGATCCTTTTGGCCGGCGCCCGCCCTACC\n",
    "#=======================================\n",
    "#\n",
    "# Aligned_sequences: 2\n",
    "# 1: EMBOSS_001\n",
    "# 2: EMBOSS_001\n",
    "# Matrix: EDNAFULL\n",
    "# Gap_penalty: 10.0\n",
    "# Extend_penalty: 0.5\n",
    "#\n",
    "# Length: 114\n",
    "# Identity:      56/114 (49.1%)\n",
    "# Similarity:    56/114 (49.1%)\n",
    "# Gaps:          44/114 (38.6%)\n",
    "# Score: 117.0\n",
    "# \n",
    "#\n",
    "#=======================================\n",
    "\n",
    "EMBOSS_001         1 GCCGGGAC-GGATG--CAACT-AGCCCCTATCAGCGTT--TGCTTTTACC     44\n",
    "                       .|||.| .||||  |.||| .||||.|.||.|||||  |..||||.||\n",
    "EMBOSS_001         1 --TGGGGCATGATGGCCCACTCGGCCCATCTCCGCGTTCCTTATTTTTCC     48\n",
    "\n",
    "EMBOSS_001        45 --GCG--TGCCAACTTCTGTGCGTC-ATTGACGATCAGCCCTTGAG----     85\n",
    "                       |||  .||||||||       || ||||  |||    ||||..|    \n",
    "EMBOSS_001        49 CGGCGGTGGCCAACTT-------TCTATTG--GAT----CCTTTTGGCCG     85\n",
    "\n",
    "EMBOSS_001        86 --------------     85\n",
    "                                   \n",
    "EMBOSS_001        86 GCGCCCGCCCTACC     99\n",
    "#---------------------------------------\n",
    "#---------------------------------------\n",
    "TRUE: TTCCCGTATGGAGTCAATCGTCAGCAAAAGAGATGATACACGGAAATTTACGACTCCGTCGTTAGCAAGCCGTACTGTTTGTGTATAAC\n",
    "PRED: CTCCTTTATTGGGGCGGTTCCATATGTCCGCCCAAGGAGGGATGGCTCCACGCCCTTTTTAAAGCCTCAGAGAACGGGCCAGCCGTTACTTAGTTTTTGGTTGGTCCA\n",
    "#=======================================\n",
    "#\n",
    "# Aligned_sequences: 2\n",
    "# 1: EMBOSS_001\n",
    "# 2: EMBOSS_001\n",
    "# Matrix: EDNAFULL\n",
    "# Gap_penalty: 10.0\n",
    "# Extend_penalty: 0.5\n",
    "#\n",
    "# Length: 115\n",
    "# Identity:      58/115 (50.4%)\n",
    "# Similarity:    58/115 (50.4%)\n",
    "# Gaps:          33/115 (28.7%)\n",
    "# Score: 83.5\n",
    "# \n",
    "#\n",
    "#=======================================\n",
    "\n",
    "EMBOSS_001         1 TTCCCGTATGGAG-----TCAAT-CGTCAGC---AAAAGAGAT-GATACA     40\n",
    "                     .|||..|||.|.|     ||.|| .|||.||   |..||.||| |.|.||\n",
    "EMBOSS_001         1 CTCCTTTATTGGGGCGGTTCCATATGTCCGCCCAAGGAGGGATGGCTCCA     50\n",
    "\n",
    "EMBOSS_001        41 CGGAAATTT--ACGACTC------CGTCGTTAGCAAGCCG-TACT--GTT     79\n",
    "                     ||....|||  |.|.|||      ||     .||.||||| ||||  |||\n",
    "EMBOSS_001        51 CGCCCTTTTTAAAGCCTCAGAGAACG-----GGCCAGCCGTTACTTAGTT     95\n",
    "\n",
    "EMBOSS_001        80 TGT-----GTATAAC     89\n",
    "                     |.|     ||..|  \n",
    "EMBOSS_001        96 TTTGGTTGGTCCA--    108\n",
    "#---------------------------------------\n",
    "#---------------------------------------\n",
    "TRUE: TCCTAGTCCAGATAATCGTGGTGGATAAGGAGAAGGTTGGGAACTCAGAAGGTTGATTCGATATGGAGAAAAACTCTGTGTACAAATGT\n",
    "PRED: CTTCCAGATACTAGTTGGGTTGGGGGATAAGGGGAGACGGTTTGGACATCCCGACGATTTTGGCTTTCGGGATATGGGGGAGAAACCTCTGTTGGTTACCCTGC\n",
    "#=======================================\n",
    "#\n",
    "# Aligned_sequences: 2\n",
    "# 1: EMBOSS_001\n",
    "# 2: EMBOSS_001\n",
    "# Matrix: EDNAFULL\n",
    "# Gap_penalty: 10.0\n",
    "# Extend_penalty: 0.5\n",
    "#\n",
    "# Length: 116\n",
    "# Identity:      63/116 (54.3%)\n",
    "# Similarity:    63/116 (54.3%)\n",
    "# Gaps:          39/116 (33.6%)\n",
    "# Score: 159.5\n",
    "# \n",
    "#\n",
    "#=======================================\n",
    "\n",
    "EMBOSS_001         1 TCCTAGTCCAGATAATCG------TGGTGGATAA--GGAGAAGGTTGGGA     42\n",
    "                       ||  ||||||||.|.|      |||.||||||  |||||.||||.|||\n",
    "EMBOSS_001         1 --CT--TCCAGATACTAGTTGGGTTGGGGGATAAGGGGAGACGGTTTGGA     46\n",
    "\n",
    "EMBOSS_001        43 -ACTCAGAAGGTT---GATT---CGATAT---GGAGAAAAACTCTGTGTA     82\n",
    "                      |..|.||.|.||   |.||   .|||||   |||| |||.||||||   \n",
    "EMBOSS_001        47 CATCCCGACGATTTTGGCTTTCGGGATATGGGGGAG-AAACCTCTGT---     92\n",
    "\n",
    "EMBOSS_001        83 CAAATGT---------     89\n",
    "                         ||.         \n",
    "EMBOSS_001        93 ----TGGTTACCCTGC    104\n",
    "#---------------------------------------\n",
    "#---------------------------------------\n",
    "TRUE: AAGGTGGTTAGAAGCCTATCAATTTCAAGGCCCTCGATGGTTGACCAGTAGGAATGACATCGTACTCGAACCACTAGTGACC\n",
    "PRED: TTAAGGGCTTGGAACGATCTCCCATTTGTGCCAGGGCCCATCGGGCTGGATTTGCCCAGTTAGAATGGGCCATCGTTACTCTAACCCCACTCGTCC\n",
    "#=======================================\n",
    "#\n",
    "# Aligned_sequences: 2\n",
    "# 1: EMBOSS_001\n",
    "# 2: EMBOSS_001\n",
    "# Matrix: EDNAFULL\n",
    "# Gap_penalty: 10.0\n",
    "# Extend_penalty: 0.5\n",
    "#\n",
    "# Length: 101\n",
    "# Identity:      65/101 (64.4%)\n",
    "# Similarity:    65/101 (64.4%)\n",
    "# Gaps:          24/101 (23.8%)\n",
    "# Score: 161.5\n",
    "# \n",
    "#\n",
    "#=======================================\n",
    "\n",
    "EMBOSS_001         1 --AAGGTGGTTAGAAGCCTAT---CAATTT----CAAGGCCC-TC--GAT     38\n",
    "                       |||| |.||.|||  |.||   |.||||    ||.||||| ||  |.|\n",
    "EMBOSS_001         1 TTAAGG-GCTTGGAA--CGATCTCCCATTTGTGCCAGGGCCCATCGGGCT     47\n",
    "\n",
    "EMBOSS_001        39 GG--TTGACCAGTAGGAAT--GACATCG-TACTCGAA--CCACTAGTGAC     81\n",
    "                     ||  |||.|||||..||||  |.||||| |||||.||  |||||.||  |\n",
    "EMBOSS_001        48 GGATTTGCCCAGTTAGAATGGGCCATCGTTACTCTAACCCCACTCGT--C     95\n",
    "\n",
    "EMBOSS_001        82 C     82\n",
    "                     |\n",
    "EMBOSS_001        96 C     96\n",
    "#---------------------------------------\n",
    "#---------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constant sequences need to be generated by directly calling `dataset.gaussian_model_fn(nt_seq)`.\n",
    "\n",
    "(Recall also that we still have `dataset.random_upsample == False`.)\n",
    "\n",
    "These sequences have a constant amount of upsampling in the kmer-to-sample conversion; the only randomness is in the gaussian distributions per 5mer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.random_upsample = True\n",
    "### set target lengths to 85:\n",
    "lengths = np.ones(batch_size) * 85\n",
    "lengths_th = torch.IntTensor(lengths.astype(np.int32))\n",
    "\n",
    "### batch of constant sequences: `seqs` = ['A'*85, 'G'*85, 'C'*85, 'T'*85, 'A'*85, 'G'*85]\n",
    "seqs = [np.ones(85, dtype=np.int32) * 1, np.ones(85, dtype=np.int32) * 2, np.ones(85, dtype=np.int32) * 3, \n",
    "        np.ones(85, dtype=np.int32) * 4, np.ones(85, dtype=np.int32) * 1, np.ones(85, dtype=np.int32) * 2]\n",
    "seq = torch.from_numpy(np.concatenate(seqs)).int()\n",
    "\n",
    "### for each sequence, sample a signal sequence and stack into a batch:\n",
    "signals = [dataset.gaussian_model_fn(sq) for sq in seqs]\n",
    "signal = torch.from_numpy(dataset.batchify(signals)).float()\n",
    "\n",
    "### get variables:\n",
    "signal_var = torch.autograd.Variable(signal, volatile=True)\n",
    "seqs_var = torch.autograd.Variable(seq, volatile=True)\n",
    "lengths_var = torch.autograd.Variable(lengths_th, volatile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lengths_th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run model on these inputs:\n",
    "probas = ctcnet(batch_norm(signal_var.unsqueeze(1)))\n",
    "transcriptions = probas.permute(2,0,1) # need seq x batch x dim\n",
    "transcription_lengths = Variable(torch.IntTensor([transcriptions.size(0)] * batch_size))\n",
    "ctc_loss = ctc_loss_fn(transcriptions, seqs_var, transcription_lengths, lengths_var)\n",
    "avg_ctc_loss = (ctc_loss / transcriptions.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"CTC Loss on whole sequence: {}\".format(ctc_loss.data[0]))\n",
    "print(\"CTC Loss, averaged per-logit: {}\".format(avg_ctc_loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print true sequences:\n",
    "true_base_sequences = split_target_seqs(seqs_var.data, lengths_var.data)\n",
    "for k in range(len(seqs)):\n",
    "    print(labels2strings(true_base_sequences[k].unsqueeze(0))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize probabilities with a softmax operation:\n",
    "temperature = 1.0 # should set this between 0->infty\n",
    "logits = transcriptions / temperature\n",
    "for k in range(len(logits)):\n",
    "    logits[k,:,:] = torch.nn.functional.softmax(logits[k,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# argmax decoding: expects (batch, seq, dim) and returns (batch, seq)\n",
    "argmax_decoded = argmax_decode(logits.permute(1,0,2).contiguous().data)\n",
    "argmax_basecalls = labels2strings(argmax_decoded)\n",
    "for k in range(len(argmax_decoded)):\n",
    "    print(argmax_basecalls[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# beam search decoded: expects (batch, dim, seq)\n",
    "beam_search_decoder = BeamSearchDecoder(batch_size=batch_size, num_labels=5, beam_width=7)\n",
    "probas, hyp_seqs = beam_search_decoder.decode(logits.permute(1, 2, 0))\n",
    "print(\"Normalized probabilities:\")\n",
    "for k in range(len(probas)):\n",
    "    print(probas[k] / logits.size(0))\n",
    "lookup_dict = {0: '', 1: 'A', 2: 'G', 3: 'C', 4: 'T', 5: '<SOS>', 6: '<EOS>'}\n",
    "for ll in range(len(hyp_seqs)):\n",
    "    print(\"\".join([lookup_dict[lbl] for lbl in hyp_seqs[ll]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
