{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RawCTCNet Benchmark/Eval with trained model: CTCLoss of approx. 0.6 (Best: 0.5548)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ptang/Desktop/pytorch_models/wavenet-speech\n",
      "/home/ptang/Desktop/pytorch_models/wavenet-speech\r\n"
     ]
    }
   ],
   "source": [
    "# switch to toplevel dir:\n",
    "%cd ~/Desktop/pytorch_models/wavenet-speech/\n",
    "!pwd\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports:\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from warpctc_pytorch import CTCLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import gaussian model, RawCTCNet, sequential decoder:\n",
    "from utils.gaussian_kmer_model import RawGaussianModelLoader\n",
    "from modules.raw_ctcnet import RawCTCNet\n",
    "from modules.sequence_decoders import argmax_decode, labels2strings, BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct data generator from gaussian model using the same parameters as we did during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create artificial data model:\n",
    "max_iterations = 1000000 # 1 million examples\n",
    "num_epochs = 100\n",
    "epoch_size = 10000\n",
    "kmer_model_path = \"utils/r9.4_450bps.5mer.template.npz\"\n",
    "batch_size = 6\n",
    "upsample_rate = 6\n",
    "min_sample_len = 80\n",
    "max_sample_len = 90\n",
    "dataset = RawGaussianModelLoader(max_iterations, num_epochs, epoch_size, kmer_model_path, batch_size=batch_size,\n",
    "                                 upsampling=upsample_rate, random_upsample=True, lengths=(min_sample_len,max_sample_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "  99.3619  101.9095  100.6845  102.5677   79.6690   83.7402   78.6904\n",
       "  86.7239   85.4107   85.8604   83.9018   97.8683   95.7186   96.9390\n",
       "  84.8001   88.4078   90.9701   83.3572   83.6874   79.0143   70.9455\n",
       " 120.9693  109.7244  114.5660  113.4171   91.3331  100.0946   89.2066\n",
       "  95.8083  103.1629   99.4160  108.6115  104.6305  102.0335  101.6434\n",
       "  81.3073   87.1847   86.8829   84.5572   84.6988   85.8178   85.1082\n",
       "[torch.FloatTensor of size 6x7]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect dataset:\n",
    "signals, bases, lengths = dataset.fetch()\n",
    "signals[:,0:7] # ~ (batch x seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct model with same parameters as during training and load saved models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build model:\n",
    "nfeats = 2048\n",
    "nhid = 512\n",
    "feature_kwidth = 3\n",
    "num_labels = 5\n",
    "num_dilation_blocks = 10\n",
    "dilations = [1, 2, 4, 8, 16] * num_dilation_blocks\n",
    "layers = [(nhid, nhid, 2, d) for d in dilations] + [(nhid, nhid, 3, d) for d in dilations]\n",
    "out_dim = 512\n",
    "is_causal = False\n",
    "ctcnet = RawCTCNet(nfeats, feature_kwidth, num_labels, layers, out_dim, input_kernel_size=2, input_dilation=1,\n",
    "                   softmax=False, causal=is_causal)\n",
    "batch_norm = torch.nn.BatchNorm1d(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load saved model parameters:\n",
    "ctcnet_save_path = \"./runs/gaussian-model/raw_ctc_net.model.adamax_lr2e_4.pth\"\n",
    "batchnorm_save_path = \"./runs/gaussian-model/raw_ctc_net.batch_norm.adamax_lr2e_4.pth\"\n",
    "map_cpu = lambda storage, loc: storage # necessary to move weights from CUDA to CPU\n",
    "ctcnet.load_state_dict(torch.load(ctcnet_save_path, map_location=map_cpu))\n",
    "batch_norm.load_state_dict(torch.load(batchnorm_save_path, map_location=map_cpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CTCLoss:\n",
    "ctc_loss_fn = CTCLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to fetch & evaluate model on data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_model():\n",
    "    # use volatile variables for better execution speed/memory usage:\n",
    "    signals, sequences, lengths = dataset.fetch()\n",
    "    signals_var = Variable(signals.data, volatile=True)\n",
    "    sequences_var = Variable(sequences.data, volatile=True)\n",
    "    lengths_var = Variable(lengths.data, volatile=True)\n",
    "    # run networks:\n",
    "    probas = ctcnet(batch_norm(signals_var.unsqueeze(1)))\n",
    "    transcriptions = probas.permute(2,0,1) # need seq x batch x dim\n",
    "    transcription_lengths = Variable(torch.IntTensor([transcriptions.size(0)] * batch_size))\n",
    "    ctc_loss = ctc_loss_fn(transcriptions, sequences_var, transcription_lengths, lengths_var)\n",
    "    avg_ctc_loss = (ctc_loss / transcriptions.size(0))\n",
    "    return (transcriptions, ctc_loss, avg_ctc_loss, sequences.data, lengths.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_target_seqs(seqs, lengths):\n",
    "    \"\"\"Split a flattened array of target sequences into their constituents.\"\"\"\n",
    "    split_seqs = []\n",
    "    labels_parsed = 0\n",
    "    for ll in lengths:\n",
    "        split_seqs.append( seqs[labels_parsed:labels_parsed+ll] )\n",
    "        labels_parsed += ll\n",
    "    return split_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate results against true sequences with argmax and beam search (run these commands in sequence a few times):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTC Loss on whole sequence: 329.54559326171875\n",
      "CTC Loss, averaged per-logit: 0.7323235273361206\n"
     ]
    }
   ],
   "source": [
    "scores, loss, avg_loss, true_seqs, true_seq_lengths = eval_model()\n",
    "print(\"CTC Loss on whole sequence: {}\".format(loss.data[0]))\n",
    "print(\"CTC Loss, averaged per-logit: {}\".format(avg_loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGGTGCACCAGTGTTTATACGACTGCGTTCTACGCCCTGAAGCTACTGAGGTAGAGCTCCGTATTACCAAGGTGCTGGTGTGGGTTTTT\n",
      "ATCTCCGATCTATAGAGTAGTTTTCACATGTGTGCGGTGCGGGTGTTAGCTCAACAGTAAGGGTGCGCACATATTCACCCCACTATT\n",
      "CCAGCTCGGAGGCAGAGGTCTCCTGCGCTGGCGAGCCCACAGACATTGAACAACTGGGCTTTCGAGACTCAACCGCACTGAC\n",
      "TGGATCACACTGGCATAACTTGAATTGAAGAACGATGACCTAAGGCACGAAGACCGTAGCGATGGTCCTGGGTATTTACG\n",
      "AGACCAGTCTGCCGTCGACTAGAGTGAATTAGTGGGTCGGTGTTGGCATGAGCGCGAGTGAGGATGGAGAGCTGCTCTCGTGCTGGTC\n",
      "ATCGAGGTTCGAAGCGTAAGTATAGTGTCGGATCACATTGAGCATCGGAGTTCGAATTGGTCATACCAACGAATCCACGTCAGATC\n"
     ]
    }
   ],
   "source": [
    "# print true sequences:\n",
    "true_base_sequences = split_target_seqs(true_seqs, true_seq_lengths)\n",
    "for k in range(len(true_base_sequences)):\n",
    "    print(labels2strings(true_base_sequences[k].unsqueeze(0))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize probabilities with a softmax operation:\n",
    "temperature = 1.0 # should set this between 0->infty\n",
    "logits = scores / temperature\n",
    "for k in range(len(logits)):\n",
    "    logits[k,:,:] = torch.nn.functional.softmax(logits[k,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCCGCCCCAGTGGTTAATACGCCTAACGTTTCTAACTCCCTGCAGGCTACTGGCGGTAAGGCTCGTCTACCAAGGGTGATGGTTGTTGGGGGCGTCG\n",
      "CCCGATCTCTCGGGCGGTTTGGATTCCCATGTTGGTTGGCGGTTCGGGGTTGTAGGGCTACAAAGGTTACGGGTGGCTCCCATCTTTCCCCAACCC\n",
      "CCCCAGGAGGCCGGAGGGCTACTGCGGCTGGGGAGGGCCCACAGCCCATTTTGAACCAACTGGGGGGCTTTTTGAGCCTCACCCGGCCCCC\n",
      "CCTGCCCTGGGGAATAACTTTTGGAATTTGGAAAGAACGGATGGGCCCTAAAGGGGCAAGAAAGCCCGGTTAGGCGGGCTGGGGAGCCTGGTTCGTTCC\n",
      "TTCCCCAGTTATGGCCGATACGCCCTCGGGAGTTGGACTGAGTGGGTCGGGTTGTTTTTGCCATGGCGGGCGCGGTGGAGGATGGGAAGGCTGGCTCTAGTGATCC\n",
      "CGGTCGGGCGAAAGCGTTAAAGTTTTATTTAGTTCTGGATCACCATTTGAAGGCCCTCGACGAGGAATTTTGGAGCCATACCCAAAGACTCCACGGTTCCACC\n"
     ]
    }
   ],
   "source": [
    "# argmax decoding: expects (batch, seq, dim) and returns (batch, seq)\n",
    "argmax_decoded = argmax_decode(logits.permute(1,0,2).contiguous().data)\n",
    "argmax_basecalls = labels2strings(argmax_decoded)\n",
    "for k in range(len(argmax_decoded)):\n",
    "    print(argmax_basecalls[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# beam search decoded: expects (batch, dim, seq)\n",
    "beam_search_decoder = BeamSearchDecoder(batch_size=batch_size, num_labels=5, beam_width=7)\n",
    "probas, hyp_seqs = beam_search_decoder.decode(logits.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized probabilities:\n",
      "0.29592603895399305\n",
      "0.29182196723090276\n",
      "0.2960758802625868\n",
      "0.29698045518663196\n",
      "0.2933231268988715\n",
      "0.2924560546875\n"
     ]
    }
   ],
   "source": [
    "print(\"Normalized probabilities:\")\n",
    "for k in range(len(probas)):\n",
    "    print(probas[k] / logits.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS>CCCGCCCCAGTGGTTAATACGCCTAACGTTTCTAACTCCCTGCAGGCTACTGGCGGTAAGGCTCGTCTACCAAGGGTGATGGTTGTTGGGGGCGTCG<EOS>\n",
      "<SOS>CCCGATCTCTCGGGCGGTTTGGATTCCCATGTTGGTTGGCGGTTCGGGGTTGTAGGGCTACAAAGGTTACGGGTGGCTCCCATCTTTCCCCAACCC<EOS>\n",
      "<SOS>CCCCAGGAGGCCGGAGGGCTACTGCGGCTGGGGAGGGCCCACAGCCCATTTTGAACCAACTGGGGGGCTTTTTGAGCCTCACCCGGCCCCC<EOS>\n",
      "<SOS>CCTGCCCTGGGGAATAACTTTTGGAATTTGGAAAGAACGGATGGGCCCTAAAGGGGCAAGAAAGCCCGGTTAGGCGGGCTGGGGAGCCTGGTTCGTTCC<EOS>\n",
      "<SOS>TTCCCCAGTTATGGCCGATACGCCCTCGGGAGTTGGACTGAGTGGGTCGGGTTGTTTTTGCCATGGCGGGCGCGGTGGAGGATGGGAAGGCTGGCTCTAGTGATCC<EOS>\n",
      "<SOS>CGGTCGGGCGAAAGCGTTAAAGTTTTATTTAGTTCTGGATCACCATTTGAAGGCCCTCGACGAGGAATTTTGGAGCCATACCCAAAGACTCCACGGTTCCACC<EOS>\n"
     ]
    }
   ],
   "source": [
    "lookup_dict = {0: '', 1: 'A', 2: 'G', 3: 'C', 4: 'T', 5: '<SOS>', 6: '<EOS>'}\n",
    "for ll in range(len(hyp_seqs)):\n",
    "    print(\"\".join([lookup_dict[lbl] for lbl in hyp_seqs[ll]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Examine model performance on noiseless/constant data:\n",
    "The CTC loss is low, but pairwise alignment gives us ~60% identity to the target sequence; let's check to see if randomness might be contributing to low pct. identity (~80%-85% is competitive).\n",
    "\n",
    "First, data without duration noise, i.e. call `dataset.fetch()` with `random_upsample==False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.random_upsample = False # temporarily turn off random lengths\n",
    "scores, loss, avg_loss, true_seqs, true_seq_lengths = eval_model()\n",
    "print(\"CTC Loss on whole sequence: {}\".format(loss.data[0]))\n",
    "print(\"CTC Loss, averaged per-logit: {}\".format(avg_loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print true sequences:\n",
    "true_base_sequences = split_target_seqs(true_seqs, true_seq_lengths)\n",
    "for k in range(len(true_base_sequences)):\n",
    "    print(labels2strings(true_base_sequences[k].unsqueeze(0))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize probabilities with a softmax operation:\n",
    "temperature = 1.0 # should set this between 0->infty\n",
    "logits = scores / temperature\n",
    "for k in range(len(logits)):\n",
    "    logits[k,:,:] = torch.nn.functional.softmax(logits[k,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argmax decoding: expects (batch, seq, dim) and returns (batch, seq)\n",
    "argmax_decoded = argmax_decode(logits.permute(1,0,2).contiguous().data)\n",
    "argmax_basecalls = labels2strings(argmax_decoded)\n",
    "for k in range(len(argmax_decoded)):\n",
    "    print(argmax_basecalls[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# beam search decoded: expects (batch, dim, seq)\n",
    "beam_search_decoder = BeamSearchDecoder(batch_size=batch_size, num_labels=5, beam_width=7)\n",
    "probas, hyp_seqs = beam_search_decoder.decode(logits.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Normalized probabilities:\")\n",
    "for k in range(len(probas)):\n",
    "    print(probas[k] / logits.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_dict = {0: '', 1: 'A', 2: 'G', 3: 'C', 4: 'T', 5: '<SOS>', 6: '<EOS>'}\n",
    "for ll in range(len(hyp_seqs)):\n",
    "    print(\"\".join([lookup_dict[lbl] for lbl in hyp_seqs[ll]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again notice that the beam-search output sequences are exactly the same as the argmaxed sequences. Outputs of an EMBOSS run on all six (true seq, pred seq) pairs:\n",
    "```\n",
    "TRUE: TATTCAACTAGCCCCAGACGGTACATCCTAGGACCGAAACATTCGTTTTGTAGAACCTCGCAATTAAACCTGTGTTGGGATGATCG\n",
    "PRED: TGAACTCGGCCCCAGCCGGGTTCCATCACTAGGCCCGACACCATTTCGTATTTGGTTGTCACCCTCGGCCCATTTAAACCTGTTGTTTTTTGGGGGATGCC\n",
    "#=======================================\n",
    "#\n",
    "# Aligned_sequences: 2\n",
    "# 1: EMBOSS_001\n",
    "# 2: EMBOSS_001\n",
    "# Matrix: EDNAFULL\n",
    "# Gap_penalty: 10.0\n",
    "# Extend_penalty: 0.5\n",
    "#\n",
    "# Length: 106\n",
    "# Identity:      68/106 (64.2%)\n",
    "# Similarity:    68/106 (64.2%)\n",
    "# Gaps:          25/106 (23.6%)\n",
    "# Score: 192.5\n",
    "# \n",
    "#\n",
    "#=======================================\n",
    "\n",
    "EMBOSS_001         1 TATTCAACT-AGCCCCAGACGG--TACATC-CTAGGACCGAAAC--ATTC     44\n",
    "                        |.|||| .|||||||.|||  |.|||| |||||.||||.||  .|||\n",
    "EMBOSS_001         1 ---TGAACTCGGCCCCAGCCGGGTTCCATCACTAGGCCCGACACCATTTC     47\n",
    "\n",
    "EMBOSS_001        45 G--TTTTGTAG--AACCTCG---CAATTAAACCTG-TGTT------GGGA     80\n",
    "                     |  |||.||.|  |.|||||   ||.||||||||| ||||      ||||\n",
    "EMBOSS_001        48 GTATTTGGTTGTCACCCTCGGCCCATTTAAACCTGTTGTTTTTTGGGGGA     97\n",
    "\n",
    "EMBOSS_001        81 TGATCG     86\n",
    "                     ||..  \n",
    "EMBOSS_001        98 TGCC--    101\n",
    "#---------------------------------------\n",
    "#---------------------------------------\n",
    "TRUE: ACTCAGAGGCAATGACGACAAAACGGGATAGCATTACTGGTGGCGGACTCGTATACCTAGGGAGCATGATGCGCATGTCATAAGAGTGG\n",
    "PRED: ACCAGGCCCATGGAAGCCCAAACGTGGAGTCCATTTCCTGGGTTGGGAGGGCCTGGTTATCCATCGGGAAGCCATGGGCTGCGCCATGGGTCCATAAGCCCAC\n",
    "#=======================================\n",
    "#\n",
    "# Aligned_sequences: 2\n",
    "# 1: EMBOSS_001\n",
    "# 2: EMBOSS_001\n",
    "# Matrix: EDNAFULL\n",
    "# Gap_penalty: 10.0\n",
    "# Extend_penalty: 0.5\n",
    "#\n",
    "# Length: 108\n",
    "# Identity:      65/108 (60.2%)\n",
    "# Similarity:    65/108 (60.2%)\n",
    "# Gaps:          24/108 (22.2%)\n",
    "# Score: 115.0\n",
    "# \n",
    "#\n",
    "#=======================================\n",
    "\n",
    "EMBOSS_001         1 ACTCAGAGGCAAT-GACGACAAAACG-GGATAGCA-TTACTGG--TGG--     43\n",
    "                     || ||| |.|.|| ||.|.|.||||| |||...|| ||.||||  |||  \n",
    "EMBOSS_001         1 AC-CAG-GCCCATGGAAGCCCAAACGTGGAGTCCATTTCCTGGGTTGGGA     48\n",
    "\n",
    "EMBOSS_001        44 CGGACTCG-TATACCTAGGGA--GCAT--GATGCG-CATG---TCATAAG     84\n",
    "                     .||.||.| |||.|.|.||||  .|||  |.|||| ||||   .||||||\n",
    "EMBOSS_001        49 GGGCCTGGTTATCCATCGGGAAGCCATGGGCTGCGCCATGGGTCCATAAG     98\n",
    "\n",
    "EMBOSS_001        85 ---AGTGG     89\n",
    "                        |.   \n",
    "EMBOSS_001        99 CCCAC---    103\n",
    "#---------------------------------------\n",
    "#---------------------------------------\n",
    "TRUE: GCCGGGACGGATGCAACTAGCCCCTATCAGCGTTTGCTTTTACCGCGTGCCAACTTCTGTGCGTCATTGACGATCAGCCCTTGAG\n",
    "PRED: TGGGGCATGATGGCCCACTCGGCCCATCTCCGCGTTCCTTATTTTTCCCGGCGGTGGCCAACTTTCTATTGGATCCTTTTGGCCGGCGCCCGCCCTACC\n",
    "#=======================================\n",
    "#\n",
    "# Aligned_sequences: 2\n",
    "# 1: EMBOSS_001\n",
    "# 2: EMBOSS_001\n",
    "# Matrix: EDNAFULL\n",
    "# Gap_penalty: 10.0\n",
    "# Extend_penalty: 0.5\n",
    "#\n",
    "# Length: 114\n",
    "# Identity:      56/114 (49.1%)\n",
    "# Similarity:    56/114 (49.1%)\n",
    "# Gaps:          44/114 (38.6%)\n",
    "# Score: 117.0\n",
    "# \n",
    "#\n",
    "#=======================================\n",
    "\n",
    "EMBOSS_001         1 GCCGGGAC-GGATG--CAACT-AGCCCCTATCAGCGTT--TGCTTTTACC     44\n",
    "                       .|||.| .||||  |.||| .||||.|.||.|||||  |..||||.||\n",
    "EMBOSS_001         1 --TGGGGCATGATGGCCCACTCGGCCCATCTCCGCGTTCCTTATTTTTCC     48\n",
    "\n",
    "EMBOSS_001        45 --GCG--TGCCAACTTCTGTGCGTC-ATTGACGATCAGCCCTTGAG----     85\n",
    "                       |||  .||||||||       || ||||  |||    ||||..|    \n",
    "EMBOSS_001        49 CGGCGGTGGCCAACTT-------TCTATTG--GAT----CCTTTTGGCCG     85\n",
    "\n",
    "EMBOSS_001        86 --------------     85\n",
    "                                   \n",
    "EMBOSS_001        86 GCGCCCGCCCTACC     99\n",
    "#---------------------------------------\n",
    "#---------------------------------------\n",
    "TRUE: TTCCCGTATGGAGTCAATCGTCAGCAAAAGAGATGATACACGGAAATTTACGACTCCGTCGTTAGCAAGCCGTACTGTTTGTGTATAAC\n",
    "PRED: CTCCTTTATTGGGGCGGTTCCATATGTCCGCCCAAGGAGGGATGGCTCCACGCCCTTTTTAAAGCCTCAGAGAACGGGCCAGCCGTTACTTAGTTTTTGGTTGGTCCA\n",
    "#=======================================\n",
    "#\n",
    "# Aligned_sequences: 2\n",
    "# 1: EMBOSS_001\n",
    "# 2: EMBOSS_001\n",
    "# Matrix: EDNAFULL\n",
    "# Gap_penalty: 10.0\n",
    "# Extend_penalty: 0.5\n",
    "#\n",
    "# Length: 115\n",
    "# Identity:      58/115 (50.4%)\n",
    "# Similarity:    58/115 (50.4%)\n",
    "# Gaps:          33/115 (28.7%)\n",
    "# Score: 83.5\n",
    "# \n",
    "#\n",
    "#=======================================\n",
    "\n",
    "EMBOSS_001         1 TTCCCGTATGGAG-----TCAAT-CGTCAGC---AAAAGAGAT-GATACA     40\n",
    "                     .|||..|||.|.|     ||.|| .|||.||   |..||.||| |.|.||\n",
    "EMBOSS_001         1 CTCCTTTATTGGGGCGGTTCCATATGTCCGCCCAAGGAGGGATGGCTCCA     50\n",
    "\n",
    "EMBOSS_001        41 CGGAAATTT--ACGACTC------CGTCGTTAGCAAGCCG-TACT--GTT     79\n",
    "                     ||....|||  |.|.|||      ||     .||.||||| ||||  |||\n",
    "EMBOSS_001        51 CGCCCTTTTTAAAGCCTCAGAGAACG-----GGCCAGCCGTTACTTAGTT     95\n",
    "\n",
    "EMBOSS_001        80 TGT-----GTATAAC     89\n",
    "                     |.|     ||..|  \n",
    "EMBOSS_001        96 TTTGGTTGGTCCA--    108\n",
    "#---------------------------------------\n",
    "#---------------------------------------\n",
    "TRUE: TCCTAGTCCAGATAATCGTGGTGGATAAGGAGAAGGTTGGGAACTCAGAAGGTTGATTCGATATGGAGAAAAACTCTGTGTACAAATGT\n",
    "PRED: CTTCCAGATACTAGTTGGGTTGGGGGATAAGGGGAGACGGTTTGGACATCCCGACGATTTTGGCTTTCGGGATATGGGGGAGAAACCTCTGTTGGTTACCCTGC\n",
    "#=======================================\n",
    "#\n",
    "# Aligned_sequences: 2\n",
    "# 1: EMBOSS_001\n",
    "# 2: EMBOSS_001\n",
    "# Matrix: EDNAFULL\n",
    "# Gap_penalty: 10.0\n",
    "# Extend_penalty: 0.5\n",
    "#\n",
    "# Length: 116\n",
    "# Identity:      63/116 (54.3%)\n",
    "# Similarity:    63/116 (54.3%)\n",
    "# Gaps:          39/116 (33.6%)\n",
    "# Score: 159.5\n",
    "# \n",
    "#\n",
    "#=======================================\n",
    "\n",
    "EMBOSS_001         1 TCCTAGTCCAGATAATCG------TGGTGGATAA--GGAGAAGGTTGGGA     42\n",
    "                       ||  ||||||||.|.|      |||.||||||  |||||.||||.|||\n",
    "EMBOSS_001         1 --CT--TCCAGATACTAGTTGGGTTGGGGGATAAGGGGAGACGGTTTGGA     46\n",
    "\n",
    "EMBOSS_001        43 -ACTCAGAAGGTT---GATT---CGATAT---GGAGAAAAACTCTGTGTA     82\n",
    "                      |..|.||.|.||   |.||   .|||||   |||| |||.||||||   \n",
    "EMBOSS_001        47 CATCCCGACGATTTTGGCTTTCGGGATATGGGGGAG-AAACCTCTGT---     92\n",
    "\n",
    "EMBOSS_001        83 CAAATGT---------     89\n",
    "                         ||.         \n",
    "EMBOSS_001        93 ----TGGTTACCCTGC    104\n",
    "#---------------------------------------\n",
    "#---------------------------------------\n",
    "TRUE: AAGGTGGTTAGAAGCCTATCAATTTCAAGGCCCTCGATGGTTGACCAGTAGGAATGACATCGTACTCGAACCACTAGTGACC\n",
    "PRED: TTAAGGGCTTGGAACGATCTCCCATTTGTGCCAGGGCCCATCGGGCTGGATTTGCCCAGTTAGAATGGGCCATCGTTACTCTAACCCCACTCGTCC\n",
    "#=======================================\n",
    "#\n",
    "# Aligned_sequences: 2\n",
    "# 1: EMBOSS_001\n",
    "# 2: EMBOSS_001\n",
    "# Matrix: EDNAFULL\n",
    "# Gap_penalty: 10.0\n",
    "# Extend_penalty: 0.5\n",
    "#\n",
    "# Length: 101\n",
    "# Identity:      65/101 (64.4%)\n",
    "# Similarity:    65/101 (64.4%)\n",
    "# Gaps:          24/101 (23.8%)\n",
    "# Score: 161.5\n",
    "# \n",
    "#\n",
    "#=======================================\n",
    "\n",
    "EMBOSS_001         1 --AAGGTGGTTAGAAGCCTAT---CAATTT----CAAGGCCC-TC--GAT     38\n",
    "                       |||| |.||.|||  |.||   |.||||    ||.||||| ||  |.|\n",
    "EMBOSS_001         1 TTAAGG-GCTTGGAA--CGATCTCCCATTTGTGCCAGGGCCCATCGGGCT     47\n",
    "\n",
    "EMBOSS_001        39 GG--TTGACCAGTAGGAAT--GACATCG-TACTCGAA--CCACTAGTGAC     81\n",
    "                     ||  |||.|||||..||||  |.||||| |||||.||  |||||.||  |\n",
    "EMBOSS_001        48 GGATTTGCCCAGTTAGAATGGGCCATCGTTACTCTAACCCCACTCGT--C     95\n",
    "\n",
    "EMBOSS_001        82 C     82\n",
    "                     |\n",
    "EMBOSS_001        96 C     96\n",
    "#---------------------------------------\n",
    "#---------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constant sequences need to be generated by directly calling `dataset.gaussian_model_fn(nt_seq)`.\n",
    "\n",
    "(Recall also that we still have `dataset.random_upsample == False`.)\n",
    "\n",
    "These sequences have a constant amount of upsampling in the kmer-to-sample conversion; the only randomness is in the gaussian distributions per 5mer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.random_upsample = True\n",
    "### set target lengths to 85:\n",
    "lengths = np.ones(batch_size) * 85\n",
    "lengths_th = torch.IntTensor(lengths.astype(np.int32))\n",
    "\n",
    "### batch of constant sequences: `seqs` = ['A'*85, 'G'*85, 'C'*85, 'T'*85, 'A'*85, 'G'*85]\n",
    "seqs = [np.ones(85, dtype=np.int32) * 1, np.ones(85, dtype=np.int32) * 2, np.ones(85, dtype=np.int32) * 3, \n",
    "        np.ones(85, dtype=np.int32) * 4, np.ones(85, dtype=np.int32) * 1, np.ones(85, dtype=np.int32) * 2]\n",
    "seq = torch.from_numpy(np.concatenate(seqs)).int()\n",
    "\n",
    "### for each sequence, sample a signal sequence and stack into a batch:\n",
    "signals = [dataset.gaussian_model_fn(sq) for sq in seqs]\n",
    "signal = torch.from_numpy(dataset.batchify(signals)).float()\n",
    "\n",
    "### get variables:\n",
    "signal_var = torch.autograd.Variable(signal, volatile=True)\n",
    "seqs_var = torch.autograd.Variable(seq, volatile=True)\n",
    "lengths_var = torch.autograd.Variable(lengths_th, volatile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 85\n",
       " 85\n",
       " 85\n",
       " 85\n",
       " 85\n",
       " 85\n",
       "[torch.IntTensor of size 6]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths_th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int32),\n",
       " array([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], dtype=int32),\n",
       " array([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=int32),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int32)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  81.7894   85.7489   81.6565  ...     0.0000    0.0000    0.0000\n",
       "  99.0241   96.5406   99.4315  ...   101.8141  101.1766    0.0000\n",
       "  71.8395   73.4045   74.0792  ...     0.0000    0.0000    0.0000\n",
       "  91.6682   93.1689   92.8809  ...    91.0465   91.1985   93.5516\n",
       "  89.6695   84.8785   83.9640  ...     0.0000    0.0000    0.0000\n",
       " 101.5442   97.6389  100.6293  ...    98.7117    0.0000    0.0000\n",
       "[torch.FloatTensor of size 6x426]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model on these inputs:\n",
    "probas = ctcnet(batch_norm(signal_var.unsqueeze(1)))\n",
    "transcriptions = probas.permute(2,0,1) # need seq x batch x dim\n",
    "transcription_lengths = Variable(torch.IntTensor([transcriptions.size(0)] * batch_size))\n",
    "ctc_loss = ctc_loss_fn(transcriptions, seqs_var, transcription_lengths, lengths_var)\n",
    "avg_ctc_loss = (ctc_loss / transcriptions.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTC Loss on whole sequence: 399.44293212890625\n",
      "CTC Loss, averaged per-logit: 0.9332779049873352\n"
     ]
    }
   ],
   "source": [
    "print(\"CTC Loss on whole sequence: {}\".format(ctc_loss.data[0]))\n",
    "print(\"CTC Loss, averaged per-logit: {}\".format(avg_ctc_loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "TTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\n"
     ]
    }
   ],
   "source": [
    "# print true sequences:\n",
    "true_base_sequences = split_target_seqs(seqs_var.data, lengths_var.data)\n",
    "for k in range(len(seqs)):\n",
    "    print(labels2strings(true_base_sequences[k].unsqueeze(0))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize probabilities with a softmax operation:\n",
    "temperature = 1.0 # should set this between 0->infty\n",
    "logits = transcriptions / temperature\n",
    "for k in range(len(logits)):\n",
    "    logits[k,:,:] = torch.nn.functional.softmax(logits[k,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTGCAAAAAAAAAACACCACAAACAACCAAAAACAAAAAACACCAAAACCCAAAACAACAAACACAAAACCAAAACCTT\n",
      "TTGGGGGGGGGGGGGGGGGGGGGGATT\n",
      "CACCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCACCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CTGGGTCACCCGTCAGTGGGAGACGGCGGGAAAGCGC\n",
      "TTAGGCACAAAAAAAACAACAAAAAAAAACACAAAAAAAAAACACCAAAAACCCCAACCAAAAAAAACACAAAACCTC\n",
      "TTAGGGGGGGGGGGGGGGGGAGT\n"
     ]
    }
   ],
   "source": [
    "# argmax decoding: expects (batch, seq, dim) and returns (batch, seq)\n",
    "argmax_decoded = argmax_decode(logits.permute(1,0,2).contiguous().data)\n",
    "argmax_basecalls = labels2strings(argmax_decoded)\n",
    "for k in range(len(argmax_decoded)):\n",
    "    print(argmax_basecalls[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized probabilities:\n",
      "0.28114994441237406\n",
      "0.27096639615353024\n",
      "0.2753838868898766\n",
      "0.28514972579813447\n",
      "0.2802821542615088\n",
      "0.27224587057238425\n",
      "<SOS>CTGCAAAAAAAAAACACCACAAACAACCAAAAACAAAAAACACCAAAACCCAAAACAACAAACACAAAACCAAAACCTT<EOS>\n",
      "<SOS>TTGGGGGGGGGGGGGGGGGGGGGGATT<EOS>\n",
      "<SOS>CACCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCACCCCCCCCCCCCCCCCCCCCCCCCCCCCC<EOS>\n",
      "<SOS>CTGGGTCACCCGTCAGTGGGAGACGGCGGGAAAGCGC<EOS>\n",
      "<SOS>TTAGGCACAAAAAAAACAACAAAAAAAAACACAAAAAAAAAACACCAAAAACCCCAACCAAAAAAAACACAAAACCTC<EOS>\n",
      "<SOS>TTAGGGGGGGGGGGGGGGGGAGT<EOS>\n"
     ]
    }
   ],
   "source": [
    "# beam search decoded: expects (batch, dim, seq)\n",
    "beam_search_decoder = BeamSearchDecoder(batch_size=batch_size, num_labels=5, beam_width=7)\n",
    "probas, hyp_seqs = beam_search_decoder.decode(logits.permute(1, 2, 0))\n",
    "print(\"Normalized probabilities:\")\n",
    "for k in range(len(probas)):\n",
    "    print(probas[k] / logits.size(0))\n",
    "lookup_dict = {0: '', 1: 'A', 2: 'G', 3: 'C', 4: 'T', 5: '<SOS>', 6: '<EOS>'}\n",
    "for ll in range(len(hyp_seqs)):\n",
    "    print(\"\".join([lookup_dict[lbl] for lbl in hyp_seqs[ll]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
