{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to overfit the whole network (WaveNet core network + CTC layer) on a single (signal, sequence) pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, import the necessities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ptang/Desktop/pytorch_models/wavenet-speech\n",
      "/home/ptang/Desktop/pytorch_models/wavenet-speech\r\n"
     ]
    }
   ],
   "source": [
    "# set cwd:\n",
    "%cd ~/Desktop/pytorch_models/wavenet-speech/\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from modules.wavenet import WaveNet\n",
    "from modules.classifier import WaveNetClassifier\n",
    "from warpctc_pytorch import CTCLoss\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now load a pair of data seq's as variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hf = h5py.File(\"data/bucketed_data.hdf5\")\n",
    "read_np = hf['bucket_0']['reads']['12'][:]\n",
    "signal_np = hf['bucket_0']['signals']['12'][:]\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.IntTensor, torch.Size([429]), \n",
       "  429\n",
       " [torch.IntTensor of size 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load read as target:\n",
    "target_seq = Variable(torch.from_numpy(read_np).int())\n",
    "target_length = Variable(torch.IntTensor([target_seq.size(0)]))\n",
    "type(target_seq.data), target_seq.size(), target_length.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4505])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encode the signal:\n",
    "signal_pt = torch.from_numpy(signal_np).long().view(1,-1)\n",
    "signal_pt.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "signal = torch.zeros(1, 256, signal_pt.size(1)).scatter_(1, signal_pt.unsqueeze(0), 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# quick look to make sure the encoding makes sense (you don't always need to run this):\n",
    "#signal[0,:,123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.FloatTensor, torch.Size([1, 256, 4505]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_seq = Variable(signal)\n",
    "type(data_seq.data), data_seq.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.LongTensor, torch.Size([1, 4505]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the cross-entropy target sequence as the dense signal:\n",
    "xe_target_seq = Variable(signal_pt)\n",
    "type(xe_target_seq.data), xe_target_seq.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's construct the model, the optimizers, and the loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model construction:\n",
    "wavenet_dils = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512,\n",
    "                1, 2, 4, 8, 16, 32, 64, 128, 256, 512,\n",
    "                1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "classifier_dils = [1, 2, 4, 8, 16, 32,\n",
    "                   1, 2, 4, 8, 16, 32,\n",
    "                   1, 2, 4, 8, 16, 32,]\n",
    "downsample_rate = 4\n",
    "num_labels = 5 # == |{A,G,C,T,-}|\n",
    "out_dim = 256\n",
    "num_levels = 256\n",
    "wavenet_layers = [(num_levels, num_levels, 2, d) for d in wavenet_dils]\n",
    "classifier_layers = [(num_levels, num_levels, 2, d) for d in classifier_dils ]\n",
    "wavenet = WaveNet(num_levels, 2, wavenet_layers, num_levels, softmax=False)\n",
    "classifier = WaveNetClassifier(num_levels, num_labels, classifier_layers, out_dim, pool_kernel_size=downsample_rate,\n",
    "                               input_kernel_size=2, input_dilation=1, softmax=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss functions:\n",
    "ctc_loss_fn = CTCLoss()\n",
    "xe_loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimizers:\n",
    "#wavenet_optimizer = optim.Adam(wavenet.parameters())\n",
    "#ctc_optimizer = optim.Adam(classifier.parameters())\n",
    "#joint_optimizer = optim.Adam(list(wavenet.parameters()) + list(classifier.parameters()),\n",
    "#                             lr=0.0001, weight_decay=0.001)\n",
    "#scheduler = ReduceLROnPlateau(joint_optimizer, 'min')\n",
    "joint_optimizer = optim.Adagrad(list(wavenet.parameters())+list(classifier.parameters()),\n",
    "                                lr=0.00003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's define convenient shorthand closures to train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict():\n",
    "    _xe_prediction = wavenet(data_seq)\n",
    "    _probs = classifier(_xe_prediction)\n",
    "    return (_xe_prediction, _probs)\n",
    "\n",
    "def train(log=False):\n",
    "    # clear gradients:\n",
    "    #wavenet_optimizer.zero_grad()\n",
    "    #ctc_optimizer.zero_grad()\n",
    "    joint_optimizer.zero_grad()\n",
    "\n",
    "    # make a prediction:\n",
    "    xe_prediction, probs = predict()\n",
    "\n",
    "    # compute cross-entropy loss against *shifted* dense signal:\n",
    "    xe_loss = 0.\n",
    "    for t in range(xe_prediction.size(2)-1):\n",
    "        xe_loss = xe_loss + xe_loss_fn(xe_prediction[:,:,t], xe_target_seq[:,t+1])\n",
    "\n",
    "    # compute CTC loss against labels:\n",
    "    probs_rearranged = probs.permute(2,0,1).contiguous()\n",
    "    prob_lengths = Variable(torch.IntTensor([probs.size(2)]))\n",
    "    target_seq_shifted = target_seq + Variable(torch.ones(target_seq.size()).int()) # because label 0 == <BLANK>\n",
    "    ctc_loss = ctc_loss_fn(probs_rearranged, target_seq_shifted, prob_lengths, target_length)\n",
    "    \n",
    "    # backprop (choose one):\n",
    "    total_loss = xe_loss + ctc_loss\n",
    "    avg_xe_loss = xe_loss / data_seq.size(2)\n",
    "    avg_ctc_loss = ctc_loss / probs.size(2)\n",
    "    avg_loss = avg_xe_loss + avg_ctc_loss\n",
    "    #avg_loss.backward()\n",
    "    avg_ctc_loss.backward()\n",
    "    \n",
    "    # apply gradient descent updates:\n",
    "    #ctc_optimizer.step()\n",
    "    #wavenet_optimizer.step()\n",
    "    joint_optimizer.step()\n",
    "    #scheduler(ctc_loss / probs.size(2))\n",
    "    \n",
    "    # return all values of interest for logging:\n",
    "    if log: return (total_loss.data[0], xe_loss.data[0], ctc_loss.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main training loop (run this cell and the next block of prediction/view cells multiple times until convergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:34<9:33:45, 34.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XE+CTC Loss Tot: 39974.9805=38791.2070+1183.7747 | Per-Sample XE: 08.6107 | Per-Label CTC: 02.7594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1000 [05:51<8:41:24, 31.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XE+CTC Loss Tot: 39400.8438=38830.5078+570.3347 | Per-Sample XE: 08.6194 | Per-Label CTC: 01.3295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 21/1000 [11:05<8:32:36, 31.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XE+CTC Loss Tot: 39255.3242=38775.4570+479.8654 | Per-Sample XE: 08.6072 | Per-Label CTC: 01.1186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 31/1000 [16:14<8:14:06, 30.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XE+CTC Loss Tot: 39156.5352=38764.1680+392.3681 | Per-Sample XE: 08.6047 | Per-Label CTC: 00.9146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 41/1000 [21:17<8:08:17, 30.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XE+CTC Loss Tot: 39125.9453=38788.3633+337.5819 | Per-Sample XE: 08.6101 | Per-Label CTC: 00.7869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 51/1000 [26:23<8:08:32, 30.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XE+CTC Loss Tot: 39043.8086=38776.1250+267.6847 | Per-Sample XE: 08.6074 | Per-Label CTC: 00.6240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 61/1000 [31:42<8:22:56, 32.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XE+CTC Loss Tot: 38997.3203=38805.2852+192.0343 | Per-Sample XE: 08.6138 | Per-Label CTC: 00.4476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 71/1000 [36:56<8:05:36, 31.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XE+CTC Loss Tot: 38983.9688=38831.0078+152.9609 | Per-Sample XE: 08.6195 | Per-Label CTC: 00.3566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 80/1000 [41:41<8:02:42, 31.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XE+CTC Loss Tot: 38923.9492=38823.3047+100.6434 | Per-Sample XE: 08.6178 | Per-Label CTC: 00.2346\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "# train loop:\n",
    "num_iterations = 1000\n",
    "log_every = 10\n",
    "for k in tqdm(range(num_iterations)):\n",
    "    if k % log_every != 0:\n",
    "        train(log=False)\n",
    "    else:\n",
    "        tot_, xe_, ctc_ = train(log=True)\n",
    "        xe_pc = xe_ / data_seq.size(2)\n",
    "        ctc_pc = ctc_ / target_seq.size(0)\n",
    "        print((\"XE+CTC Loss Tot: {0:07.4f}={1:07.4f}+{2:07.4f} | \" + \\\n",
    "               \"Per-Sample XE: {3:07.4f} | Per-Label CTC: {4:07.4f}\").format(tot_, xe_, ctc_, xe_pc, ctc_pc))\n",
    "        if (ctc_pc < 0.3):\n",
    "            print(\"Early stopping!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk:\n",
    "torch.save(wavenet.state_dict(), \"./ipynbs/wavenet_model.overfit.pth\")\n",
    "torch.save(classifier.state_dict(), \"./ipynbs/classifier_model.overfit.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These next few cells inspect predictions; run these after each run of the training block above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate fresh predictions:\n",
    "_, ctc_preds = predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called: A | Proba: 0.8391\n",
      "Called: C | Proba: 0.8950\n",
      "Called: A | Proba: 0.9490\n",
      "Called: T | Proba: 0.6770\n",
      "Called: T | Proba: 0.7874\n",
      "Called: A | Proba: 0.9319\n",
      "Called: C | Proba: 0.9179\n",
      "Called: T | Proba: 0.8514\n",
      "Called: T | Proba: 0.7239\n",
      "Called: T | Proba: 0.6321\n",
      "Called: C | Proba: 0.9145\n",
      "Called: G | Proba: 0.8421\n",
      "Called: T | Proba: 0.8787\n",
      "Called: T | Proba: 0.9059\n",
      "Called: G | Proba: 0.9116\n",
      "Called: A | Proba: 0.6849\n",
      "Called: T | Proba: 0.7350\n",
      "Called: T | Proba: 0.7878\n",
      "Called: A | Proba: 0.9633\n",
      "Called: C | Proba: 0.9220\n",
      "Called: G | Proba: 0.7141\n",
      "Called: T | Proba: 0.7282\n",
      "Called: T | Proba: 0.7633\n",
      "Called: A | Proba: 0.8934\n",
      "Called: T | Proba: 0.9409\n",
      "Called: T | Proba: 0.7898\n",
      "Called: G | Proba: 0.8589\n",
      "Called: C | Proba: 0.8013\n",
      "Called: T | Proba: 0.8448\n",
      "Called: G | Proba: 0.8460\n",
      "Called: A | Proba: 0.9286\n",
      "Called: A | Proba: 0.7578\n",
      "Called: A | Proba: 0.7427\n",
      "Called: T | Proba: 0.8852\n",
      "Called: C | Proba: 0.8919\n",
      "Called: C | Proba: 0.8216\n",
      "Called: T | Proba: 0.9485\n",
      "Called: C | Proba: 0.8678\n",
      "Called: G | Proba: 0.7746\n",
      "Called: A | Proba: 0.9244\n",
      "Called: A | Proba: 0.7062\n",
      "Called: A | Proba: 0.8440\n",
      "Called: G | Proba: 0.8778\n",
      "Called: C | Proba: 0.8016\n",
      "Called: G | Proba: 0.9140\n",
      "Called: A | Proba: 0.9354\n",
      "Called: T | Proba: 0.9337\n",
      "Called: A | Proba: 0.9202\n",
      "Called: T | Proba: 0.9030\n",
      "Called: T | Proba: 0.7014\n",
      "Called: C | Proba: 0.8150\n",
      "Called: C | Proba: 0.9577\n",
      "Called: T | Proba: 0.7503\n",
      "Called: C | Proba: 0.8066\n",
      "Called: T | Proba: 0.5166\n",
      "Called: T | Proba: 0.9345\n",
      "Called: T | Proba: 0.7758\n",
      "Called: T | Proba: 0.8877\n",
      "Called: G | Proba: 0.8395\n",
      "Called: C | Proba: 0.8844\n",
      "Called: A | Proba: 0.8727\n",
      "Called: G | Proba: 0.8813\n",
      "Called: A | Proba: 0.9250\n",
      "Called: T | Proba: 0.9606\n",
      "Called: T | Proba: 0.9203\n",
      "Called: T | Proba: 0.4909\n",
      "Called: T | Proba: 0.7950\n",
      "Called: T | Proba: 0.7721\n",
      "Called: A | Proba: 0.6427\n",
      "Called: A | Proba: 0.8899\n",
      "Called: C | Proba: 0.9022\n",
      "Called: A | Proba: 0.8904\n",
      "Called: A | Proba: 0.8938\n",
      "Called: A | Proba: 0.8714\n",
      "Called: A | Proba: 0.5951\n",
      "Called: G | Proba: 0.8546\n",
      "Called: T | Proba: 0.8936\n",
      "Called: G | Proba: 0.6410\n",
      "Called: G | Proba: 0.6228\n",
      "Called: T | Proba: 0.8339\n",
      "Called: T | Proba: 0.9491\n",
      "Called: T | Proba: 0.9530\n",
      "Called: T | Proba: 0.9518\n",
      "Called: C | Proba: 0.9064\n",
      "Called: A | Proba: 0.8031\n",
      "Called: A | Proba: 0.7274\n",
      "Called: A | Proba: 0.9567\n",
      "Called: A | Proba: 0.8245\n",
      "Called: C | Proba: 0.9622\n",
      "Called: T | Proba: 0.8643\n",
      "Called: G | Proba: 0.9013\n",
      "Called: C | Proba: 0.7959\n",
      "Called: T | Proba: 0.9528\n",
      "Called: C | Proba: 0.8899\n",
      "Called: T | Proba: 0.9257\n",
      "Called: A | Proba: 0.7338\n",
      "Called: T | Proba: 0.6646\n",
      "Called: T | Proba: 0.8116\n",
      "Called: C | Proba: 0.8323\n",
      "Called: A | Proba: 0.8185\n",
      "Called: A | Proba: 0.8866\n",
      "Called: A | Proba: 0.5945\n",
      "Called: A | Proba: 0.5697\n",
      "Called: G | Proba: 0.9727\n",
      "Called: A | Proba: 0.8515\n",
      "Called: A | Proba: 0.8231\n",
      "Called: A | Proba: 0.8745\n",
      "Called: G | Proba: 0.8655\n",
      "Called: G | Proba: 0.8963\n",
      "Called: T | Proba: 0.8524\n",
      "Called: T | Proba: 0.9516\n",
      "Called: C | Proba: 0.8142\n",
      "Called: C | Proba: 0.7703\n",
      "Called: A | Proba: 0.8127\n",
      "Called: G | Proba: 0.8418\n",
      "Called: C | Proba: 0.8085\n",
      "Called: T | Proba: 0.8478\n",
      "Called: C | Proba: 0.9204\n",
      "Called: T | Proba: 0.7675\n",
      "Called: C | Proba: 0.8572\n",
      "Called: T | Proba: 0.9215\n",
      "Called: A | Proba: 0.9243\n",
      "Called: T | Proba: 0.8191\n",
      "Called: T | Proba: 0.6627\n",
      "Called: T | Proba: 0.6570\n",
      "Called: T | Proba: 0.8375\n",
      "Called: A | Proba: 0.8220\n",
      "Called: G | Proba: 0.8675\n",
      "Called: T | Proba: 0.7785\n",
      "Called: T | Proba: 0.9674\n",
      "Called: G | Proba: 0.9183\n",
      "Called: A | Proba: 0.9013\n",
      "Called: G | Proba: 0.8143\n",
      "Called: G | Proba: 0.7553\n",
      "Called: G | Proba: 0.7732\n",
      "Called: C | Proba: 0.8429\n",
      "Called: A | Proba: 0.8964\n",
      "Called: C | Proba: 0.9033\n",
      "Called: A | Proba: 0.8160\n",
      "Called: T | Proba: 0.8880\n",
      "Called: C | Proba: 0.8229\n",
      "Called: A | Proba: 0.7936\n",
      "Called: C | Proba: 0.9376\n",
      "Called: A | Proba: 0.8310\n",
      "Called: A | Proba: 0.9341\n",
      "Called: A | Proba: 0.9121\n",
      "Called: T | Proba: 0.8597\n",
      "Called: A | Proba: 0.8120\n",
      "Called: A | Proba: 0.6331\n",
      "Called: A | Proba: 0.7629\n",
      "Called: T | Proba: 0.7479\n",
      "Called: A | Proba: 0.9142\n",
      "Called: A | Proba: 0.8282\n",
      "Called: A | Proba: 0.8355\n",
      "Called: C | Proba: 0.6190\n",
      "Called: C | Proba: 0.6979\n",
      "Called: T | Proba: 0.8375\n",
      "Called: T | Proba: 0.9443\n",
      "Called: T | Proba: 0.7563\n",
      "Called: A | Proba: 0.5701\n",
      "Called: A | Proba: 0.8255\n",
      "Called: C | Proba: 0.7971\n",
      "Called: A | Proba: 0.8303\n",
      "Called: G | Proba: 0.5812\n",
      "Called: A | Proba: 0.9252\n",
      "Called: A | Proba: 0.8948\n",
      "Called: T | Proba: 0.7867\n",
      "Called: G | Proba: 0.4844\n",
      "Called: G | Proba: 0.7795\n",
      "Called: C | Proba: 0.8750\n",
      "Called: C | Proba: 0.8046\n",
      "Called: T | Proba: 0.5026\n",
      "Called: T | Proba: 0.9312\n",
      "Called: C | Proba: 0.9074\n",
      "Called: T | Proba: 0.9049\n",
      "Called: G | Proba: 0.9159\n",
      "Called: T | Proba: 0.9338\n",
      "Called: C | Proba: 0.9164\n",
      "Called: T | Proba: 0.9173\n",
      "Called: A | Proba: 0.9476\n",
      "Called: G | Proba: 0.9044\n",
      "Called: T | Proba: 0.9588\n",
      "Called: T | Proba: 0.8366\n",
      "Called: T | Proba: 0.7515\n",
      "Called: T | Proba: 0.9747\n",
      "Called: C | Proba: 0.5122\n",
      "Called: C | Proba: 0.4087\n",
      "Called: A | Proba: 0.8875\n",
      "Called: C | Proba: 0.9083\n",
      "Called: G | Proba: 0.7217\n",
      "Called: G | Proba: 0.9156\n",
      "Called: G | Proba: 0.9041\n",
      "Called: A | Proba: 0.9141\n",
      "Called: A | Proba: 0.8914\n",
      "Called: G | Proba: 0.8805\n",
      "Called: A | Proba: 0.7707\n",
      "Called: T | Proba: 0.8916\n",
      "Called: C | Proba: 0.9702\n",
      "Called: G | Proba: 0.9744\n",
      "Called: T | Proba: 0.9249\n",
      "Called: A | Proba: 0.9374\n",
      "Called: T | Proba: 0.9156\n",
      "Called: T | Proba: 0.7294\n",
      "Called: T | Proba: 0.8443\n",
      "Called: C | Proba: 0.7506\n",
      "Called: C | Proba: 0.9074\n",
      "Called: T | Proba: 0.9378\n",
      "Called: T | Proba: 0.8417\n",
      "Called: T | Proba: 0.9311\n",
      "Called: T | Proba: 0.9671\n",
      "Called: C | Proba: 0.8540\n",
      "Called: A | Proba: 0.7719\n",
      "Called: C | Proba: 0.6487\n",
      "Called: C | Proba: 0.8345\n",
      "Called: C | Proba: 0.8260\n",
      "Called: A | Proba: 0.8078\n",
      "Called: A | Proba: 0.6761\n",
      "Called: T | Proba: 0.9610\n",
      "Called: A | Proba: 0.8812\n",
      "Called: C | Proba: 0.7104\n",
      "Called: G | Proba: 0.7770\n",
      "Called: C | Proba: 0.8966\n",
      "Called: C | Proba: 0.9366\n",
      "Called: T | Proba: 0.9463\n",
      "Called: G | Proba: 0.8220\n",
      "Called: A | Proba: 0.9440\n",
      "Called: A | Proba: 0.9440\n",
      "Called: A | Proba: 0.9303\n",
      "Called: G | Proba: 0.8827\n",
      "Called: C | Proba: 0.9219\n",
      "Called: G | Proba: 0.8499\n",
      "Called: C | Proba: 0.9269\n",
      "Called: T | Proba: 0.8355\n",
      "Called: C | Proba: 0.8587\n",
      "Called: A | Proba: 0.8436\n",
      "Called: A | Proba: 0.9010\n",
      "Called: A | Proba: 0.7740\n",
      "Called: T | Proba: 0.9377\n",
      "Called: G | Proba: 0.9624\n",
      "Called: T | Proba: 0.9740\n",
      "Called: C | Proba: 0.7695\n",
      "Called: C | Proba: 0.9423\n",
      "Called: A | Proba: 0.7134\n",
      "Called: T | Proba: 0.9133\n",
      "Called: A | Proba: 0.7938\n",
      "Called: T | Proba: 0.7425\n",
      "Called: T | Proba: 0.8695\n",
      "Called: T | Proba: 0.9909\n",
      "Called: C | Proba: 0.9199\n",
      "Called: A | Proba: 0.8724\n",
      "Called: G | Proba: 0.8795\n",
      "Called: A | Proba: 0.9564\n",
      "Called: T | Proba: 0.8328\n",
      "Called: A | Proba: 0.8515\n",
      "Called: C | Proba: 0.7138\n",
      "Called: C | Proba: 0.7269\n",
      "Called: T | Proba: 0.9347\n",
      "Called: G | Proba: 0.8001\n",
      "Called: C | Proba: 0.8069\n",
      "Called: A | Proba: 0.4926\n",
      "Called: A | Proba: 0.7962\n",
      "Called: A | Proba: 0.9008\n",
      "Called: A | Proba: 0.7403\n",
      "Called: G | Proba: 0.7970\n",
      "Called: A | Proba: 0.7482\n",
      "Called: G | Proba: 0.9163\n",
      "Called: T | Proba: 0.8492\n",
      "Called: T | Proba: 0.6350\n",
      "Called: G | Proba: 0.9069\n",
      "Called: T | Proba: 0.9658\n",
      "Called: T | Proba: 0.8688\n",
      "Called: T | Proba: 0.8461\n",
      "Called: C | Proba: 0.5101\n",
      "Called: C | Proba: 0.4744\n",
      "Called: C | Proba: 0.6780\n",
      "Called: A | Proba: 0.7900\n",
      "Called: A | Proba: 0.7528\n",
      "Called: G | Proba: 0.8757\n",
      "Called: C | Proba: 0.7987\n",
      "Called: C | Proba: 0.8752\n",
      "Called: T | Proba: 0.9273\n",
      "Called: G | Proba: 0.9545\n",
      "Called: C | Proba: 0.9376\n",
      "Called: T | Proba: 0.9305\n",
      "Called: C | Proba: 0.9223\n",
      "Called: T | Proba: 0.9042\n",
      "Called: A | Proba: 0.9772\n",
      "Called: T | Proba: 0.9470\n",
      "Called: G | Proba: 0.8309\n",
      "Called: A | Proba: 0.9428\n",
      "Called: G | Proba: 0.9618\n",
      "Called: G | Proba: 0.8891\n",
      "Called: A | Proba: 0.7623\n",
      "Called: A | Proba: 0.8867\n",
      "Called: T | Proba: 0.9373\n",
      "Called: G | Proba: 0.7839\n",
      "Called: C | Proba: 0.8647\n",
      "Called: T | Proba: 0.9874\n",
      "Called: C | Proba: 0.7149\n",
      "Called: C | Proba: 0.5434\n",
      "Called: A | Proba: 0.8085\n",
      "Called: G | Proba: 0.9650\n",
      "Called: C | Proba: 0.9532\n",
      "Called: T | Proba: 0.8347\n",
      "Called: C | Proba: 0.8343\n",
      "Called: T | Proba: 0.6923\n",
      "Called: G | Proba: 0.9045\n",
      "Called: T | Proba: 0.9340\n",
      "Called: A | Proba: 0.8122\n",
      "Called: G | Proba: 0.9067\n",
      "Called: A | Proba: 0.9264\n",
      "Called: T | Proba: 0.4922\n",
      "Called: T | Proba: 0.6522\n",
      "Called: G | Proba: 0.8008\n",
      "Called: A | Proba: 0.7934\n",
      "Called: A | Proba: 0.7505\n",
      "Called: T | Proba: 0.9139\n",
      "Called: A | Proba: 0.7110\n",
      "Called: G | Proba: 0.8917\n",
      "Called: A | Proba: 0.9610\n",
      "Called: C | Proba: 0.8547\n",
      "Called: G | Proba: 0.8891\n",
      "Called: T | Proba: 0.8196\n",
      "Called: T | Proba: 0.8968\n",
      "Called: A | Proba: 0.8995\n",
      "Called: C | Proba: 0.9569\n",
      "Called: A | Proba: 0.9036\n",
      "Called: A | Proba: 0.9427\n",
      "Called: A | Proba: 0.6578\n",
      "Called: A | Proba: 0.8385\n",
      "Called: G | Proba: 0.7801\n",
      "Called: T | Proba: 0.6194\n",
      "Called: T | Proba: 0.9315\n",
      "Called: T | Proba: 0.8598\n",
      "Called: C | Proba: 0.8766\n",
      "Called: C | Proba: 0.5229\n",
      "Called: G | Proba: 0.6789\n",
      "Called: G | Proba: 0.8935\n",
      "Called: A | Proba: 0.8949\n",
      "Called: T | Proba: 0.5090\n",
      "Called: G | Proba: 0.8170\n",
      "Called: C | Proba: 0.8083\n",
      "Called: T | Proba: 0.8361\n",
      "Called: T | Proba: 0.6105\n",
      "Called: G | Proba: 0.9576\n",
      "Called: C | Proba: 0.9698\n",
      "Called: T | Proba: 0.6992\n",
      "Called: T | Proba: 0.7088\n",
      "Called: G | Proba: 0.8732\n",
      "Called: T | Proba: 0.9824\n",
      "Called: A | Proba: 0.9387\n",
      "Called: T | Proba: 0.8471\n",
      "Called: C | Proba: 0.8931\n",
      "Called: T | Proba: 0.7362\n",
      "Called: C | Proba: 0.7771\n",
      "Called: C | Proba: 0.9171\n",
      "Called: T | Proba: 0.8882\n",
      "Called: T | Proba: 0.9221\n",
      "Called: T | Proba: 0.6827\n",
      "Called: T | Proba: 0.8586\n",
      "Called: T | Proba: 0.9338\n",
      "Called: A | Proba: 0.8395\n",
      "Called: T | Proba: 0.7792\n",
      "Called: A | Proba: 0.7929\n",
      "Called: A | Proba: 0.7137\n",
      "Called: T | Proba: 0.7662\n",
      "Called: T | Proba: 0.8053\n",
      "Called: A | Proba: 0.9269\n",
      "Called: A | Proba: 0.6759\n",
      "Called: T | Proba: 0.6377\n",
      "Called: T | Proba: 0.8843\n",
      "Called: A | Proba: 0.7878\n",
      "Called: G | Proba: 0.8233\n",
      "Called: T | Proba: 0.7245\n",
      "Called: C | Proba: 0.9044\n",
      "Called: C | Proba: 0.8299\n",
      "Called: G | Proba: 0.9343\n",
      "Called: G | Proba: 0.4542\n",
      "Called: T | Proba: 0.9364\n",
      "Called: T | Proba: 0.8560\n",
      "Called: T | Proba: 0.8455\n",
      "Called: C | Proba: 0.8108\n",
      "Called: C | Proba: 0.7846\n",
      "Called: A | Proba: 0.8105\n",
      "Called: A | Proba: 0.8076\n",
      "Called: G | Proba: 0.9211\n",
      "Called: C | Proba: 0.8846\n",
      "Called: A | Proba: 0.8571\n",
      "Called: G | Proba: 0.7676\n",
      "Called: A | Proba: 0.7399\n",
      "Called: A | Proba: 0.7611\n",
      "Called: T | Proba: 0.9024\n",
      "Called: C | Proba: 0.7618\n",
      "Called: C | Proba: 0.8551\n",
      "Called: T | Proba: 0.5185\n",
      "Called: T | Proba: 0.7355\n",
      "Called: C | Proba: 0.8948\n",
      "Called: A | Proba: 0.9783\n",
      "Called: A | Proba: 0.6698\n",
      "Called: A | Proba: 0.7790\n",
      "Called: G | Proba: 0.9109\n",
      "Called: C | Proba: 0.7798\n",
      "Called: T | Proba: 0.7456\n",
      "Called: T | Proba: 0.7317\n",
      "Called: G | Proba: 0.5761\n",
      "Called: G | Proba: 0.8385\n",
      "Called: C | Proba: 0.8013\n",
      "Called: A | Proba: 0.9031\n",
      "Called: A | Proba: 0.6873\n",
      "Called: A | Proba: 0.8285\n",
      "Called: T | Proba: 0.7284\n",
      "Called: A | Proba: 0.9075\n",
      "Called: T | Proba: 0.9865\n",
      "Called: C | Proba: 0.7539\n",
      "Called: C | Proba: 0.9547\n",
      "Called: A | Proba: 0.9038\n",
      "Called: C | Proba: 0.8381\n",
      "Called: T | Proba: 0.7376\n",
      "Called: T | Proba: 0.9052\n",
      "Called: T | Proba: 0.8660\n",
      "Called: G | Proba: 0.7171\n",
      "Called: C | Proba: 0.8930\n",
      "Called: A | Proba: 0.7982\n",
      "Called: G | Proba: 0.9237\n",
      "Called: A | Proba: 0.8603\n",
      "Called: T | Proba: 0.5830\n",
      "Called: T | Proba: 0.7179\n",
      "Called: C | Proba: 0.4891\n",
      "Called: C | Proba: 0.7200\n",
      "Called: A | Proba: 0.9175\n",
      "Called: C | Proba: 0.9020\n",
      "Called: G | Proba: 0.9604\n",
      "Called: A | Proba: 0.9053\n",
      "Called: A | Proba: 0.9099\n",
      "Called: A | Proba: 0.8246\n",
      "Called: A | Proba: 0.5802\n",
      "Called: A | Proba: 0.9598\n",
      "Called: C | Proba: 0.9851\n",
      "Called: G | Proba: 0.9225\n",
      "Called: G | Proba: 0.9253\n",
      "Called: T | Proba: 0.9373\n",
      "Called: G | Proba: 0.8930\n",
      "Called: T | Proba: 0.9142\n",
      "Called: T | Proba: 0.9533\n",
      "Called: T | Proba: 0.6805\n",
      "Called: C | Proba: 0.9011\n",
      "Called: A | Proba: 0.7591\n",
      "Called: G | Proba: 0.8967\n",
      "Called: A | Proba: 0.8388\n",
      "Called: A | Proba: 0.9265\n",
      "Called: C | Proba: 0.8985\n",
      "Called: T | Proba: 0.9293\n",
      "Called: G | Proba: 0.9220\n",
      "Called: C | Proba: 0.8972\n",
      "Called: T | Proba: 0.7658\n",
      "Called: C | Proba: 0.7506\n",
      "Called: C | Proba: 0.8656\n",
      "Called: T | Proba: 0.9313\n",
      "Called: T | Proba: 0.9882\n",
      "Called: C | Proba: 0.9125\n",
      "Called: A | Proba: 0.5084\n",
      "Called: A | Proba: 0.9377\n",
      "Called: A | Proba: 0.8456\n",
      "Called: G | Proba: 0.9359\n",
      "Called: G | Proba: 0.9281\n",
      "Called: C | Proba: 0.9091\n",
      "Called: A | Proba: 0.8797\n",
      "Called: G | Proba: 0.8687\n",
      "Called: T | Proba: 0.9152\n",
      "Called: A | Proba: 0.8506\n",
      "Called: G | Proba: 0.8292\n",
      "Called: T | Proba: 0.9015\n",
      "Called: T | Proba: 0.9806\n",
      "Called: T | Proba: 0.6634\n",
      "Called: T | Proba: 0.7401\n",
      "Called: T | Proba: 0.4830\n",
      "Called: C | Proba: 0.7543\n",
      "Called: A | Proba: 0.9460\n",
      "Called: A | Proba: 0.9140\n",
      "Called: A | Proba: 0.9165\n",
      "Called: A | Proba: 0.6762\n",
      "Called: T | Proba: 0.7432\n",
      "Called: T | Proba: 0.9300\n",
      "Called: T | Proba: 0.9128\n",
      "Called: T | Proba: 0.9334\n",
      "Called: C | Proba: 0.9322\n"
     ]
    }
   ],
   "source": [
    "# print outputs:\n",
    "_lookup_ = {0: '<BLANK>', 1: 'A', 2: 'G', 3: 'C', 4: 'T'}\n",
    "print_blanks = False\n",
    "pred_labels = []\n",
    "for k in range(ctc_preds.size(2)):\n",
    "    logit, label = torch.max(torch.nn.functional.softmax(ctc_preds[0,:,k]), dim=0)\n",
    "    logit_py = float(logit.data[0])\n",
    "    label_py = _lookup_[int(label.data[0])]\n",
    "    if (not print_blanks) and (label_py == '<BLANK>'): continue\n",
    "    print(\"Called: {0} | Proba: {1:1.4f}\".format(label_py, logit_py))\n",
    "    pred_labels.append(label_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACATTACTTTCGTTGATTACGTTATTGCTGAAATCCTCGAAAGCGATATTCCTCTTTTGCAGATTTTTAACAAAAGTGGTTTTCAAAACTGCTCTATTCAAAAGAAAGGTTCCAGCTCTCTATTTTAGTTGAGGGCACATCACAAATAAATAAACCTTTAACAGAATGGCCTTCTGTCTAGTTTTCCACGGGAAGATCGTATTTCCTTTTCACCCAATACGCCTGAAAGCGCTCAAATGTCCATATTTCAGATACCTGCAAAAGAGTTGTTTCCCAAGCCTGCTCTATGAGGAATGCTCCAGCTCTGTAGATTGAATAGACGTTACAAAAGTTTCCGGATGCTTGCTTGTATCTCCTTTTTATAATTAATTAGTCCGGTTTCCAAGCAGAATCCTTCAAAGCTTGGCAAATATCCACTTTGCAGATTCCACGAAAAACGGTGTTTCAGAACTGCTCCTTCAAAGGCAGTAGTTTTTCAAAATTTTC\n"
     ]
    }
   ],
   "source": [
    "print(\"\".join(pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACATACTTCGTTGATTACGTATTGCTGAAATCCTCGAAGCGATATCCTCTTGCAGATTTTACAAAGTGTTTCAAAACTGCTCTATCAAAGAAAGGTTCCAGCTCTCTATTAGTTGAGGCACATCACAAATAATAAACTACAGAATGCTTCTGTCTAGTTTTCACGGGAAGATCGTATTTCCTTTTCACCCATACGCCTGAAAGCGCTCAAATGTCATATTCAGATACTGCAAAGAGTGTTTCCAGCCTGCTCTATGAGGAATGCTCAGCTCTGTAGATTGAATAGACGTACAAAGTTTCTGAGATGCTGCTGTATCTCCTTTTATATATAGTCCGTTTCCAGCAGAATCCTCAAAGCTGGCAAATATCCACTTGCAGATTCACGAAAACGGTGTTTCAGAACTGCTCCTTCAAGGCAGTAGTTTCAATC\n"
     ]
    }
   ],
   "source": [
    "print(\"\".join(_lookup_[ix] for ix in list(target_seq.data + torch.ones(target_seq.size()).int())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CTC debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1.4519\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# taken from the WarpCTC tests:\n",
    "_activs = Variable(torch.FloatTensor([[[-10., -9., -8., -7., -6.]]]).transpose(0, 1).contiguous(), requires_grad=True)\n",
    "_activ_sizes = Variable(torch.IntTensor([1]))\n",
    "_labels = Variable(torch.IntTensor([3]))\n",
    "_label_sizes = Variable(torch.IntTensor([1]))\n",
    "print(ctc_loss_fn(_activs, _labels, _activ_sizes, _label_sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
